{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1 Part 1: Calculating Running Average and Standard Deviation\n",
    "\n",
    "One of the analytics use case to be tackled is a real-time alerting system:\n",
    "\n",
    "`Hourly consumption for a household is higher than 1 standard deviation of that household's historical mean consumption for that hour.`\n",
    "\n",
    "To do that, I split the use case into three parts. \n",
    "\n",
    "First, read the data stream from the `readings_prepared` topic, group them by `house_id` and the **hour** of the `reading_ts`, and calculate the statistics (average and standard deviation) of that group. Everytime the value changes, or every small period of time, we persist the change inside a database / stream. Let's name the target store `alert_1_stats`. This first task is reflected on this notebook. \n",
    "\n",
    "The second part is reading the data stream from both `readings_prepared` and `alert_1_stats` and use them to detect anomalies, and store the anomalies in a persistent data store. \n",
    "\n",
    "The third task is a developign scheduled job to check that data store to see if there are anomalies and send alerts to the related parties. **The last 2 tasks will have their own notebook / code.**\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "Import all the required libraries and set the stream configuration variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://spark-alert-1-stats-m:8088/proxy/application_1583161277657_0002\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = yarn, app id = application_1583161277657_0002)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "import java.sql.Timestamp\n",
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode, GroupState}\n",
       "import org.apache.spark.sql.streaming.Trigger\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "import java.sql.Timestamp\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode, GroupState}\n",
    "import org.apache.spark.sql.streaming.Trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kafkaBootstrapServer: String = kafka-m:9092\n",
       "kafkaSourceTopic: String = readings_prepared\n",
       "kafkaTargetTopic: String = alert_1_stats\n",
       "checkpointLocation: String = /tmp\n",
       "triggerTime: String = 1 minute\n",
       "deduplicateWindow: String = 1 minute\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kafkaBootstrapServer = \"kafka-m:9092\"\n",
    "val kafkaSourceTopic = \"readings_prepared\"\n",
    "val kafkaTargetTopic = \"alert_1_stats\"\n",
    "val checkpointLocation = \"/tmp\"\n",
    "val triggerTime = \"1 minute\"\n",
    "val deduplicateWindow = \"1 minute\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define The Required Schema and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mySchema: org.apache.spark.sql.types.StructType = StructType(StructField(message_id,StringType,false), StructField(reading_ts,TimestampType,false), StructField(reading_value,FloatType,false), StructField(reading_type,IntegerType,false), StructField(plug_id,IntegerType,false), StructField(household_id,IntegerType,false), StructField(house_id,IntegerType,false))\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// This will be used to give the source `readings_prepared` stream data a schema\n",
    "val mySchema = StructType(Seq(\n",
    "    StructField(\"message_id\", StringType, false),\n",
    "    StructField(\"reading_ts\", TimestampType, false),\n",
    "    StructField(\"reading_value\", FloatType, false),\n",
    "    StructField(\"reading_type\", IntegerType, false),\n",
    "    StructField(\"plug_id\", IntegerType, false),\n",
    "    StructField(\"household_id\", IntegerType, false),\n",
    "    StructField(\"house_id\", IntegerType, false)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class ReadingsInput\n",
       "defined class StatsState\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Somehow case classes need to be defined in a separate cell\n",
    "// This will be used in the stateful streaming calculation as the input rows\n",
    "case class ReadingsInput(\n",
    "    house_id: Int, \n",
    "    hour: Int, \n",
    "    reading_value: Float,\n",
    "    reading_ts: java.sql.Timestamp \n",
    ")\n",
    "\n",
    "// This will be used in the stateful streaming calculation as the state store schema\n",
    "case class StatsState(\n",
    "    house_id: Int,\n",
    "    hour: Int,\n",
    "    mean: Float,\n",
    "    m2: Float,\n",
    "    variance: Float,\n",
    "    std_dev: Float,\n",
    "    count: Long, \n",
    "    last_ts: java.sql.Timestamp // Will be used in stream-to-stream join\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and Parse The Input Stream\n",
    "\n",
    "Only take the Current Load readings (`reading_type = 1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readings: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [message_id: string, reading_ts: timestamp ... 5 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val readings = spark\n",
    "    .readStream \n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n",
    "    .option(\"subscribe\", kafkaSourceTopic)\n",
    "    .option(\"failOnDataLoss\", false)\n",
    "    .load()\n",
    "    .selectExpr(\"CAST(value AS STRING)\")\n",
    "    .select(from_json($\"value\", mySchema).as(\"data\"))\n",
    "    .select($\"data.*\")\n",
    "    .withWatermark(\"reading_ts\", deduplicateWindow) \n",
    "    .dropDuplicates()\n",
    "    .filter($\"reading_type\" === 1) // Only take the \"current load\" measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peek at The Input Data Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "streamQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@767b3bb8\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val streamQuery = readings.writeStream.format(\"memory\").queryName(\"readings\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Getting offsets from KafkaV2[Subscribe[readings_prepared]]\",\n",
       "  \"isDataAvailable\" : false,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------+------------+-------+------------+--------+\n",
      "|message_id|         reading_ts|reading_value|reading_type|plug_id|household_id|house_id|\n",
      "+----------+-------------------+-------------+------------+-------+------------+--------+\n",
      "| 118504114|2013-09-01 17:53:00|          0.0|           1|      0|           0|       9|\n",
      "| 115998526|2013-09-01 17:27:20|          0.0|           1|      0|           0|       0|\n",
      "| 116872749|2013-09-01 17:36:20|          0.0|           1|      1|           0|       9|\n",
      "| 118371375|2013-09-01 17:51:40|          0.0|           1|      2|           0|       4|\n",
      "| 117153098|2013-09-01 17:39:20|          0.0|           1|      0|           0|       5|\n",
      "| 117306922|2013-09-01 17:41:00|       50.627|           1|      0|           0|       8|\n",
      "| 118503302|2013-09-01 17:53:00|       41.056|           1|      2|           0|       0|\n",
      "| 118868629|2013-09-01 17:56:40|      122.738|           1|      1|           0|       1|\n",
      "| 119725890|2013-09-01 18:05:20|          0.0|           1|      0|           0|       6|\n",
      "| 119590206|2013-09-01 18:04:00|          0.0|           1|      2|           0|       7|\n",
      "| 119496230|2013-09-01 18:03:00|          0.0|           1|      1|           0|       6|\n",
      "| 119465366|2013-09-01 18:02:40|          0.0|           1|      2|           0|       9|\n",
      "| 119136988|2013-09-01 17:59:20|          0.0|           1|      0|           0|       4|\n",
      "| 119333968|2013-09-01 18:01:20|        40.97|           1|      2|           0|       0|\n",
      "| 119300642|2013-09-01 18:01:00|          0.0|           1|      2|           0|       2|\n",
      "| 119336636|2013-09-01 18:01:20|          0.0|           1|      0|           0|       9|\n",
      "| 119234296|2013-09-01 18:00:20|      140.243|           1|      1|           0|       1|\n",
      "| 119689344|2013-09-01 18:05:00|          0.0|           1|      1|           0|       3|\n",
      "| 119465250|2013-09-01 18:02:40|          0.0|           1|      0|           0|       5|\n",
      "| 119590291|2013-09-01 18:04:00|       62.252|           1|      0|           0|       8|\n",
      "+----------+-------------------+-------------+------------+-------+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Thread.sleep(60000)\n",
    "spark.sql(\"select * from readings\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: org.apache.spark.sql.streaming.StreamingQueryProgress = null\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// streamQuery.stop()\n",
    "streamQuery.lastProgress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate The Running Average and Standard Deviation\n",
    "\n",
    "To complete the first task, one thing to keep in mind is that we are calculating a running/moving average/standard deviation over an unbounded stream of data. With that in mind, the typical algorithm of calculating average over a limited amount of bounded/batch data, `average = sum_of_values / population_size`, won't work. \n",
    "\n",
    "Why? Since we need to sum all values on a stream, we need to store the state of the sum at every given point in time, and as the stream goes on and one, this can lead to numeric overflow issues. Instead of doing that, we can incrementally calculate average by keeping track of the current average and the current population size / record count, and then adjust as new data comes.  We can do that using [Welford's online algorithm](https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_online_algorithm).\n",
    "\n",
    "#### Storing The Running Statistics\n",
    "\n",
    "Once we have a stream of running average and standard deviation, ideally, I want to output and store the stats in a dataset whose very low latency lookup speed like HBase or Google BigTable. However, due to technical issues with HBase's Spark library, I store the data back to Kafka on the `alert_1_stats` stream. I did not want to spend too much time trying to resolve the issue so I decided to drop it.\n",
    "\n",
    "This [blog post](https://medium.com/@robbinjain19/challenges-faced-while-integrating-apache-spark-with-hbase-and-the-solution-5c1c8a068808) suggests that I downgrade to Spark 2.1.0 to resolve the HBase connector issue, but I need to use the `foreachBatch` functionality from Spark 2.4.0 and stream-to-stream join from Spark 2.3.0.\n",
    "\n",
    "\n",
    "### Define the Functions to Be Used in MapGroupsWithState\n",
    "\n",
    "I created 2 functions: `updateHouseHourStats(state, input)` and `updateAllHouseHourStats(key, input, groupState)`. The latter will be called by Spark's `mapGroupsWithState` function and call the former to work on individual rows to update the state.\n",
    "\n",
    "The case states \"`that household's historical mean consumption for that hour.`\", hence here, the grouping keys used are `house_id` and the `hour(reading_ts)`. I looked at the data, the `household_id` is always 0 so it does not make sense to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateHouseHourStats: (state: StatsState, input: ReadingsInput)StatsState\n",
       "updateAllHouseHourStats: (key: (Int, Int), inputs: Iterator[ReadingsInput], groupState: org.apache.spark.sql.streaming.GroupState[StatsState])StatsState\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/**\n",
    " * To be called by updateAllHouseHourStats\n",
    " */\n",
    "def updateHouseHourStats(state: StatsState, input: ReadingsInput): StatsState = {\n",
    "    // This is implementing Welford's moving average / standard deviation algorithm\n",
    "    val newCount = state.count + 1\n",
    "    val delta = input.reading_value - state.mean\n",
    "    val newMean = state.mean + (delta / newCount)\n",
    "    val newDelta = input.reading_value - newMean\n",
    "    val newM2 = state.m2 + (delta * newDelta)\n",
    "    // Calculate Sample variance, state.count == newCount - 1\n",
    "    val newVariance = if(newCount > 1) { newM2 / state.count } else { 0 } \n",
    "    val newStdDev = if(newCount > 1) { Math.sqrt(newVariance).toFloat } else { 0 }\n",
    "\n",
    "    return StatsState(\n",
    "        state.house_id,\n",
    "        state.hour,\n",
    "        newMean,\n",
    "        newM2,\n",
    "        newVariance,\n",
    "        newStdDev,\n",
    "        newCount,\n",
    "        input.reading_ts\n",
    "    )\n",
    "}\n",
    "\n",
    "/**\n",
    " * To be called by mapGroupWithState\n",
    " */\n",
    "def updateAllHouseHourStats(\n",
    "  key: (Int, Int),\n",
    "  inputs: Iterator[ReadingsInput],\n",
    "  groupState: GroupState[StatsState]\n",
    ") : StatsState = {\n",
    "    // Get previous state if exists, else create a new empty state\n",
    "    var currentStatsState = groupState.getOption.getOrElse {\n",
    "        new StatsState(\n",
    "            key._1,\n",
    "            key._2,\n",
    "            0,  // Mean\n",
    "            0,  // m2\n",
    "            0,  // Variance\n",
    "            0,  // STD Dev\n",
    "            0,   // Count \n",
    "            null\n",
    "        )\n",
    "    }\n",
    "    // Loop over the inputs in this microbatch\n",
    "    for (input <- inputs) {\n",
    "        currentStatsState = updateHouseHourStats(currentStatsState, input)\n",
    "    }\n",
    "    // Update the current state\n",
    "    groupState.update(currentStatsState)\n",
    "    // Return the current state to the stream\n",
    "    return currentStatsState\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send Update to Kafka\n",
    "\n",
    "Pull the trigger every arbitrary **1 minute**, as set in `triggerTime`, since we may want to wait until we accumulate enough data for statistics calculation. We are using the `GroupStateTimeout.NoTimeout` option -- meaning that the stats state will continually gets updated until the stream is stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stats: org.apache.spark.sql.Dataset[StatsState] = [house_id: int, hour: int ... 6 more fields]\n",
       "query: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@13e3be6\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stats = readings\n",
    "    .select($\"house_id\", hour($\"reading_ts\").alias(\"hour\"), $\"reading_value\", $\"reading_ts\")\n",
    "    .toDF()\n",
    "    .as[ReadingsInput]\n",
    "    .groupByKey(\n",
    "        input => (input.house_id, input.hour)\n",
    "    )\n",
    "    .mapGroupsWithState(GroupStateTimeout.NoTimeout)(updateAllHouseHourStats)\n",
    "    \n",
    "val query = stats\n",
    "    .selectExpr(\"CONCAT(house_id, '-', hour, '-', last_ts) AS key\", \"CAST(to_json(struct(*)) AS STRING) AS value\")\n",
    "    .writeStream\n",
    "    .outputMode(\"update\")\n",
    "    .format(\"kafka\")\n",
    "    .trigger(Trigger.ProcessingTime(triggerTime))\n",
    "    .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n",
    "    .option(\"checkpointLocation\", checkpointLocation)\n",
    "    .option(\"topic\", kafkaTargetTopic)\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Processing new data\",\n",
       "  \"isDataAvailable\" : true,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Thread.sleep(10000)\n",
    "query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: org.apache.spark.sql.streaming.StreamingQueryProgress =\n",
       "{\n",
       "  \"id\" : \"8e479727-56f4-41c9-ab47-d5e50904d707\",\n",
       "  \"runId\" : \"6f8234eb-ce83-4e55-a34a-625defdc8a24\",\n",
       "  \"name\" : null,\n",
       "  \"timestamp\" : \"2020-03-02T15:29:00.001Z\",\n",
       "  \"batchId\" : 1,\n",
       "  \"numInputRows\" : 33000,\n",
       "  \"inputRowsPerSecond\" : 549.9908334861086,\n",
       "  \"processedRowsPerSecond\" : 692.7097546128172,\n",
       "  \"durationMs\" : {\n",
       "    \"addBatch\" : 46391,\n",
       "    \"getBatch\" : 0,\n",
       "    \"getEndOffset\" : 0,\n",
       "    \"queryPlanning\" : 674,\n",
       "    \"setOffsetRange\" : 1,\n",
       "    \"triggerExecution\" : 47639,\n",
       "    \"walCommit\" : 426\n",
       "  },\n",
       "  \"eventTime\" : {\n",
       "    \"avg\" : \"2013-09-01T03:48:44.664Z\",\n",
       "    \"max\" : \"2013-09-01T18:08:00.000Z\",\n",
       "    \"min\" : \"2013-08-31T22:00:20.000Z\",\n",
       "    \"watermark\" : \"1970-01-01T00:00:00.000Z\"\n",
       "  },\n",
       "  \"stateOperators\" : [ {\n",
       "    \"numRowsTotal\" : 110..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// query.stop()\n",
    "query.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}