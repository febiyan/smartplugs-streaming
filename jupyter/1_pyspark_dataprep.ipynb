{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Data Preparation\n",
    "\n",
    "This notebook showcases the simple data preparation to be done on the raw streaming data on `readings_raw` topic using PySpark. I am using Spark's structured streaming as it is the new way to do Streaming in Spark, and offers SQL-like transformations which will ease development. However, there is one small thing that worries me when it comes to Spark and Kafka integration -- even the latest, 2.4.x, Spark version is still using the old Kafka broker 0.10 version.\n",
    "\n",
    "The topic streams a comma-separated value format, which we will need to parse, deduplicate, and drop if any of the field values are null. The resulting clean data will then streamed back to `readings_prepared` Kafka topic, to be consumed by Spark streaming jobs downstream.\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we import required libraries and define variables to be used to control the stream I/O. The Kafka Bootstrap Server lives on `kafka-m` hostname, which is resolvable in Google Cloud VPC network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaBootstrapServer = \"kafka-m:9092\"\n",
    "kafkaSourceTopic = \"readings_raw\"\n",
    "kafkaTargetTopic = \"readings_prepared\"\n",
    "checkpointLocation = \"/tmp\"\n",
    "deduplicateWindow = \"1 minute\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Stream and Prepare The Data\n",
    "\n",
    "### Connect to Kafka and Subscribe to `readings_raw` Topic\n",
    "\n",
    "Connect to Kafka and create the streaming DataFrame. We then take a look at the schema produced. In Jupyter Notebooks or PySpark shell, the `spark` variable is created by default. There is no need to create another one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafkaSourceDF = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer) \\\n",
    "    .option(\"subscribe\", kafkaSourceTopic) \\\n",
    "    .option(\"failOnDataLoss\", False) \\\n",
    "    .load()\n",
    "kafkaSourceDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peek at the Source Streaming Data\n",
    "\n",
    "We can peek at the Kafka source stream, `kafkaSourceDF`, to ensure that Spark is reading the stream properly. There were times when, somehow, it didn't read data from the stream although the producer is producing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaSourceQuery = kafkaSourceDF.writeStream\\\n",
    "    .queryName(\"kafka_source\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .start()\n",
    "# Wait 5 seconds before querying\n",
    "time.sleep(5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "|key|value|topic|partition|offset|timestamp|timestampType|\n",
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from kafka_source\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Processing new data',\n",
       " 'isDataAvailable': True,\n",
       " 'isTriggerActive': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafkaSourceQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print last progress, if necessary, and stop the \n",
    "# If we continue the query, we will eat the Driver's memory unnecessarily.\n",
    "# kafkaSourceQuery.lastProgress\n",
    "kafkaSourceQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the Comma-separated Values and Output an Array\n",
    "\n",
    "We then use the `split()` function to split the `value` column, whose the comma separated values into a column with values in an array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the CSV value from KafkaDF and turn it into an array\n",
    "csvDF = kafkaSourceDF.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), \"data STRING\").alias(\"value\"),\n",
    "    col(\"timestamp\")\n",
    ").select(\n",
    "    split(col(\"value.data\"), \",\").alias(\"value\"),\n",
    "    col(\"timestamp\")\n",
    ")\n",
    "csvDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the Array Column and Sanitise\n",
    "\n",
    "Drop duplicates within **arbitrary** 1 minute watermark. What it means is that Spark keeps the state of the stream, then use the state to deduplicate records with `reading_ts` no later than 1 minute window backwards from the max seen `reading_ts`. The window duration is configurable, but the longer it takes, the more memory it consumes to keep the state in-memory. The `message_id` column is set to string to be able to be used as Kafka topic key.\n",
    "\n",
    "See [Spark Structured Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- message_id: string (nullable = true)\n",
      " |-- reading_ts: timestamp (nullable = true)\n",
      " |-- reading_value: float (nullable = true)\n",
      " |-- reading_type: integer (nullable = true)\n",
      " |-- plug_id: integer (nullable = true)\n",
      " |-- household_id: integer (nullable = true)\n",
      " |-- house_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parse the CSV and drop the duplicate values\n",
    "# The stream from Kafka may deliver a message at least once\n",
    "readingsDF = csvDF\\\n",
    "    .withColumn(\"message_id\", col(\"value\").getItem(0).cast(\"string\"))\\\n",
    "    .withColumn(\"reading_ts\", col(\"value\").getItem(1).cast(\"integer\").cast(\"timestamp\"))\\\n",
    "    .withColumn(\"reading_value\", col(\"value\").getItem(2).cast(\"float\"))\\\n",
    "    .withColumn(\"reading_type\", col(\"value\").getItem(3).cast(\"integer\"))\\\n",
    "    .withColumn(\"plug_id\", col(\"value\").getItem(4).cast(\"integer\"))\\\n",
    "    .withColumn(\"household_id\", col(\"value\").getItem(5).cast(\"integer\"))\\\n",
    "    .withColumn(\"house_id\", trim(col(\"value\").getItem(6)).cast(\"integer\"))\\\n",
    "    .drop(\"value\")\\\n",
    "    .dropna()\\\n",
    "    .withWatermark(\"reading_ts\", deduplicateWindow)\\\n",
    "    .dropDuplicates()\n",
    "\n",
    "readingsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peek the Parsed and Prepared Data Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "readingsQuery = readingsDF.writeStream\\\n",
    "    .queryName(\"readings\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Getting offsets from KafkaV2[Subscribe[readings_raw]]',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': True}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readingsQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-------------------+-------------+------------+-------+------------+--------+\n",
      "|           timestamp|message_id|         reading_ts|reading_value|reading_type|plug_id|household_id|house_id|\n",
      "+--------------------+----------+-------------------+-------------+------------+-------+------------+--------+\n",
      "|2020-03-04 09:18:...|  31658518|2013-09-01 03:19:00|          0.0|           1|      0|           0|       2|\n",
      "|2020-03-04 09:18:...|  32092789|2013-09-01 03:23:20|          0.0|           1|      1|           0|       7|\n",
      "|2020-03-04 09:18:...|  33025115|2013-09-01 03:32:40|        3.216|           0|      1|           0|       9|\n",
      "|2020-03-04 09:18:...|  33357749|2013-09-01 03:36:00|          0.0|           1|      2|           0|       9|\n",
      "|2020-03-04 09:18:...|  33456068|2013-09-01 03:37:00|        9.636|           1|      2|           0|       0|\n",
      "|2020-03-04 09:18:...|  33755379|2013-09-01 03:40:00|       35.105|           0|      1|           0|       1|\n",
      "|2020-03-04 09:18:...|  34221820|2013-09-01 03:44:40|         3.16|           1|      1|           0|       1|\n",
      "|2020-03-04 09:19:...|  35120685|2013-09-01 03:53:40|       11.779|           0|      2|           0|       0|\n",
      "|2020-03-04 09:19:...|  35122369|2013-09-01 03:53:40|        6.345|           1|      1|           0|       8|\n",
      "|2020-03-04 09:19:...|  35222177|2013-09-01 03:54:40|          0.0|           1|      2|           0|       9|\n",
      "|2020-03-04 09:19:...|  35387312|2013-09-01 03:56:20|        0.788|           0|      0|           0|       2|\n",
      "|2020-03-04 09:19:...|  35755293|2013-09-01 04:00:00|          0.0|           1|      0|           0|       5|\n",
      "|2020-03-04 09:19:...|  36587275|2013-09-01 04:08:20|          0.0|           1|      0|           0|       9|\n",
      "|2020-03-04 09:19:...|  36620606|2013-09-01 04:08:40|         2.25|           0|      1|           0|       5|\n",
      "|2020-03-04 09:19:...|  36686905|2013-09-01 04:09:20|        3.219|           0|      0|           0|       5|\n",
      "|2020-03-04 09:19:...|  37018355|2013-09-01 04:12:40|        0.161|           0|      2|           0|       2|\n",
      "|2020-03-04 09:19:...|  37685861|2013-09-01 04:19:20|        0.788|           0|      0|           0|       4|\n",
      "|2020-03-04 09:19:...|  38117719|2013-09-01 04:23:40|          0.0|           1|      0|           0|       4|\n",
      "|2020-03-04 09:18:...|  31959658|2013-09-01 03:22:00|       35.137|           0|      1|           0|       8|\n",
      "|2020-03-04 09:18:...|  32026059|2013-09-01 03:22:40|       35.137|           0|      1|           0|       8|\n",
      "+--------------------+----------+-------------------+-------------+------------+-------+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sleep 40 seconds because we need to wait for the streaming state to initialise\n",
    "time.sleep(40)\n",
    "spark.sql(\"select * from readings\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to stop this later\n",
    "readingsQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Back Prepared Data to Kafka\n",
    "\n",
    "Now that the data is clean, we write back to Kafka -- to the `readings_prepared` topic to be consumed by alerting and ingestion streaming jobs downstream. Write as soon as data is available, without a time-based trigger. Using timestamp as key in case I need to restart streaming jobs while iterating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaWriteQuery = readingsDF.selectExpr(\"CAST(timestamp AS STRING) AS key\", \"CAST(to_json(struct(*)) AS STRING) AS value\")\\\n",
    "    .writeStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\\\n",
    "    .option(\"checkpointLocation\", checkpointLocation)\\\n",
    "    .option(\"topic\", kafkaTargetTopic)\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the status and last progress of the write to Kafka using `.lastProgress` and `status`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '6715fe46-e91d-41ac-a294-3aa04ab8f020',\n",
       " 'runId': '41874cf1-0545-4d28-915a-228234a71ad5',\n",
       " 'name': None,\n",
       " 'timestamp': '2020-03-04T09:19:27.252Z',\n",
       " 'batchId': 0,\n",
       " 'numInputRows': 0,\n",
       " 'processedRowsPerSecond': 0.0,\n",
       " 'durationMs': {'addBatch': 11734,\n",
       "  'getBatch': 0,\n",
       "  'getEndOffset': 0,\n",
       "  'queryPlanning': 106,\n",
       "  'setOffsetRange': 499,\n",
       "  'triggerExecution': 12859,\n",
       "  'walCommit': 46},\n",
       " 'eventTime': {'watermark': '1970-01-01T00:00:00.000Z'},\n",
       " 'stateOperators': [{'numRowsTotal': 0,\n",
       "   'numRowsUpdated': 0,\n",
       "   'memoryUsedBytes': 44599,\n",
       "   'customMetrics': {'loadedMapCacheHitCount': 0,\n",
       "    'loadedMapCacheMissCount': 0,\n",
       "    'stateOnCurrentVersionSizeBytes': 15799}}],\n",
       " 'sources': [{'description': 'KafkaV2[Subscribe[readings_raw]]',\n",
       "   'startOffset': None,\n",
       "   'endOffset': {'readings_raw': {'0': 26000}},\n",
       "   'numInputRows': 0,\n",
       "   'processedRowsPerSecond': 0.0}],\n",
       " 'sink': {'description': 'org.apache.spark.sql.kafka010.KafkaSourceProvider@275f381f'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(15)\n",
    "kafkaWriteQuery.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Processing new data',\n",
       " 'isDataAvailable': True,\n",
       " 'isTriggerActive': True}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafkaWriteQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaWriteQuery.lastProgress\n",
    "# kafkaWriteQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}