{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Data Preparation\n",
    "\n",
    "This notebook showcases the simple data preparation to be done on the raw streaming data on `readings_raw` topic using PySpark. I am using Spark's structured streaming as it is the new way to do Streaming in Spark, and offers SQL-like transformations which will ease development. However, there is one small thing that worries me when it comes to Spark and Kafka integration -- even the latest, 2.4.x, Spark version is still using the old Kafka broker 0.10 version.\n",
    "\n",
    "The topic streams a comma-separated value format, which we will need to parse, deduplicate, and drop if any of the field values are null. The resulting clean data will then streamed back to `readings_prepared` Kafka topic, to be consumed by Spark streaming jobs downstream.\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we import required libraries and define variables to be used to control the stream I/O. The Kafka Bootstrap Server lives on `kafka-m` hostname, which is resolvable in Google Cloud VPC network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaBootstrapServer = \"kafka-m:9092\"\n",
    "kafkaSourceTopic = \"readings_raw\"\n",
    "kafkaTargetTopic = \"readings_prepared\"\n",
    "checkpointLocation = \"/tmp\"\n",
    "deduplicateWindow = \"1 minute\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Stream and Prepare The Data\n",
    "\n",
    "### Connect to Kafka and Subscribe to `readings_raw` Topic\n",
    "\n",
    "Connect to Kafka and create the streaming DataFrame. We then take a look at the schema produced. In Jupyter Notebooks or PySpark shell, the `spark` variable is created by default. There is no need to create another one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafkaSourceDF = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer) \\\n",
    "    .option(\"subscribe\", kafkaSourceTopic) \\\n",
    "    .option(\"failOnDataLoss\", False) \\\n",
    "    .load()\n",
    "kafkaSourceDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peek at the Source Streaming Data\n",
    "\n",
    "We can peek at the Kafka source stream, `kafkaSourceDF`, to ensure that Spark is reading the stream properly. There were times when, somehow, it didn't read data from the stream although the producer is producing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaSourceQuery = kafkaSourceDF.writeStream\\\n",
    "    .queryName(\"kafka_source\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------+---------+------+--------------------+-------------+\n",
      "| key|               value|       topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+------------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 64 61 74 6...|readings_raw|        0| 74000|2020-03-02 15:26:...|            0|\n",
      "|null|[7B 22 64 61 74 6...|readings_raw|        0| 74001|2020-03-02 15:26:...|            0|\n",
      "|null|[7B 22 64 61 74 6...|readings_raw|        0| 74002|2020-03-02 15:26:...|            0|\n",
      "|null|[7B 22 64 61 74 6...|readings_raw|        0| 74003|2020-03-02 15:26:...|            0|\n",
      "|null|[7B 22 64 61 74 6...|readings_raw|        0| 74004|2020-03-02 15:26:...|            0|\n",
      "|null|[7B 22 64 61 74 6...|readings_raw|        0| 74005|2020-03-02 15:26:...|            0|\n",
      "|null|[7B 22 64 61 74 6...|readings_raw|        0| 74006|2020-03-02 15:26:...|            0|\n",
      "|null|[7B 22 64 61 74 6...|readings_raw|        0| 74007|2020-03-02 15:26:...|            0|\n",
      "|null|[7B 22 64 61 74 6...|readings_raw|        0| 74008|2020-03-02 15:26:...|            0|\n",
      "|null|[7B 22 64 61 74 6...|readings_raw|        0| 74009|2020-03-02 15:26:...|            0|\n",
      "|null|[7B 22 64 61 74 6...|readings_raw|        0| 74010|2020-03-02 15:26:...|            0|\n",
      "|null|[7B 22 64 61 74 6...|readings_raw|        0| 74011|2020-03-02 15:26:...|            0|\n",
      "|null|[7B 22 64 61 74 6...|readings_raw|        0| 74012|2020-03-02 15:26:...|            0|\n",
      "|null|[7B 22 64 61 74 6...|readings_raw|        0| 74013|2020-03-02 15:26:...|            0|\n",
      "|null|[7B 22 64 61 74 6...|readings_raw|        0| 74014|2020-03-02 15:26:...|            0|\n",
      "|null|[7B 22 64 61 74 6...|readings_raw|        0| 74015|2020-03-02 15:26:...|            0|\n",
      "|null|[7B 22 64 61 74 6...|readings_raw|        0| 74016|2020-03-02 15:26:...|            0|\n",
      "|null|[7B 22 64 61 74 6...|readings_raw|        0| 74017|2020-03-02 15:26:...|            0|\n",
      "|null|[7B 22 64 61 74 6...|readings_raw|        0| 74018|2020-03-02 15:26:...|            0|\n",
      "|null|[7B 22 64 61 74 6...|readings_raw|        0| 74019|2020-03-02 15:26:...|            0|\n",
      "+----+--------------------+------------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Wait 5 seconds before querying\n",
    "time.sleep(5);\n",
    "spark.sql(\"select * from kafka_source\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Processing new data',\n",
       " 'isDataAvailable': True,\n",
       " 'isTriggerActive': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafkaSourceQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print last progress, if necessary, and stop the \n",
    "# If we continue the query, we will eat the Driver's memory unnecessarily.\n",
    "# kafkaSourceQuery.lastProgress\n",
    "kafkaSourceQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the Comma-separated Values and Output an Array\n",
    "\n",
    "We then use the `split()` function to split the `value` column, whose the comma separated values into a column with values in an array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the CSV value from KafkaDF and turn it into an array\n",
    "csvDF = kafkaSourceDF.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), \"data STRING\").alias(\"value\"),\n",
    "    col(\"timestamp\")\n",
    ").select(\n",
    "    split(col(\"value.data\"), \",\").alias(\"value\"),\n",
    "    col(\"timestamp\")\n",
    ")\n",
    "csvDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the Array Column and Sanitise\n",
    "\n",
    "Drop duplicates within **arbitrary** 1 minute watermark. What it means is that Spark keeps the state of the stream, then use the state to deduplicate records with `reading_ts` no later than 1 minute window backwards from the max seen `reading_ts`. The window duration is configurable, but the longer it takes, the more memory it consumes to keep the state in-memory. The `message_id` column is set to string to be able to be used as Kafka topic key.\n",
    "\n",
    "See [Spark Structured Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- message_id: string (nullable = true)\n",
      " |-- reading_ts: timestamp (nullable = true)\n",
      " |-- reading_value: float (nullable = true)\n",
      " |-- reading_type: integer (nullable = true)\n",
      " |-- plug_id: integer (nullable = true)\n",
      " |-- household_id: integer (nullable = true)\n",
      " |-- house_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parse the CSV and drop the duplicate values\n",
    "# The stream from Kafka may deliver a message at least once\n",
    "readingsDF = csvDF\\\n",
    "    .withColumn(\"message_id\", col(\"value\").getItem(0).cast(\"string\"))\\\n",
    "    .withColumn(\"reading_ts\", col(\"value\").getItem(1).cast(\"integer\").cast(\"timestamp\"))\\\n",
    "    .withColumn(\"reading_value\", col(\"value\").getItem(2).cast(\"float\"))\\\n",
    "    .withColumn(\"reading_type\", col(\"value\").getItem(3).cast(\"integer\"))\\\n",
    "    .withColumn(\"plug_id\", col(\"value\").getItem(4).cast(\"integer\"))\\\n",
    "    .withColumn(\"household_id\", col(\"value\").getItem(5).cast(\"integer\"))\\\n",
    "    .withColumn(\"house_id\", trim(col(\"value\").getItem(6)).cast(\"integer\"))\\\n",
    "    .drop(\"value\")\\\n",
    "    .dropna()\\\n",
    "    .withWatermark(\"reading_ts\", deduplicateWindow)\\\n",
    "    .dropDuplicates()\n",
    "\n",
    "readingsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peek the Parsed and Prepared Data Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "readingsQuery = readingsDF.writeStream\\\n",
    "    .queryName(\"readings\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Getting offsets from KafkaV2[Subscribe[readings_raw]]',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': True}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readingsQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-------------------+-------------+------------+-------+------------+--------+\n",
      "|           timestamp|message_id|         reading_ts|reading_value|reading_type|plug_id|household_id|house_id|\n",
      "+--------------------+----------+-------------------+-------------+------------+-------+------------+--------+\n",
      "|2020-03-02 15:26:...|  14240628|2013-09-01 00:22:40|        3.216|           0|      1|           0|       3|\n",
      "|2020-03-02 15:26:...|  14274618|2013-09-01 00:23:00|        0.788|           0|      0|           0|       4|\n",
      "|2020-03-02 15:26:...|  14408067|2013-09-01 00:24:20|        0.788|           0|      0|           0|       4|\n",
      "|2020-03-02 15:26:...|  14408242|2013-09-01 00:24:20|          0.0|           1|      1|           0|       7|\n",
      "|2020-03-02 15:26:...|  15572218|2013-09-01 00:36:00|          0.0|           1|      2|           0|       7|\n",
      "|2020-03-02 15:26:...|  15704595|2013-09-01 00:37:20|        3.351|           1|      1|           0|       1|\n",
      "|2020-03-02 15:26:...|  15871287|2013-09-01 00:39:00|        2.806|           1|      1|           0|       1|\n",
      "|2020-03-02 15:26:...|  16005317|2013-09-01 00:40:20|          0.0|           1|      1|           0|       3|\n",
      "|2020-03-02 15:26:...|  16571952|2013-09-01 00:46:00|        0.161|           0|      0|           0|       9|\n",
      "|2020-03-02 15:26:...|  16737181|2013-09-01 00:47:40|        0.355|           0|      0|           0|       3|\n",
      "|2020-03-02 15:26:...|  16871327|2013-09-01 00:49:00|         2.25|           0|      1|           0|       7|\n",
      "|2020-03-02 15:26:...|  16904196|2013-09-01 00:49:20|       15.359|           1|      0|           0|       8|\n",
      "|2020-03-02 15:26:...|  17336394|2013-09-01 00:53:40|          0.0|           1|      2|           0|       2|\n",
      "|2020-03-02 15:26:...|  17470731|2013-09-01 00:55:00|        0.161|           0|      0|           0|       9|\n",
      "|2020-03-02 15:26:...|  19008859|2013-09-01 01:12:20|          0.0|           1|      2|           0|       9|\n",
      "|2020-03-02 15:27:...|  19939852|2013-09-01 01:21:40|        0.161|           0|      2|           0|       2|\n",
      "|2020-03-02 15:27:...|  19941181|2013-09-01 01:21:40|          0.0|           1|      0|           0|       5|\n",
      "|2020-03-02 15:27:...|  20274253|2013-09-01 01:25:00|         2.25|           0|      1|           0|       7|\n",
      "|2020-03-02 15:27:...|  20373966|2013-09-01 01:26:00|        0.161|           0|      2|           0|       9|\n",
      "|2020-03-02 15:27:...|  20606958|2013-09-01 01:28:20|         2.25|           0|      1|           0|       5|\n",
      "+--------------------+----------+-------------------+-------------+------------+-------+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sleep 40 seconds because we need to wait for the streaming state to initialise\n",
    "time.sleep(40)\n",
    "spark.sql(\"select * from readings\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to stop this later\n",
    "readingsQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Back Prepared Data to Kafka\n",
    "\n",
    "Now that the data is clean, we write back to Kafka -- to the `readings_prepared` topic to be consumed by alerting and ingestion streaming jobs downstream. Write as soon as data is available, without a time-based trigger. Using timestamp as key in case I need to restart streaming jobs while iterating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaWriteQuery = readingsDF.selectExpr(\"CAST(timestamp AS STRING) AS key\", \"CAST(to_json(struct(*)) AS STRING) AS value\")\\\n",
    "    .writeStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\\\n",
    "    .option(\"checkpointLocation\", checkpointLocation)\\\n",
    "    .option(\"topic\", kafkaTargetTopic)\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the status and last progress of the write to Kafka using `.lastProgress` and `status`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0226cc16-5616-4ca7-a484-423e99021fe1',\n",
       " 'runId': '93ce24a1-a2dc-4369-869e-491ccbf0b441',\n",
       " 'name': None,\n",
       " 'timestamp': '2020-03-02T15:28:26.107Z',\n",
       " 'batchId': 0,\n",
       " 'numInputRows': 0,\n",
       " 'processedRowsPerSecond': 0.0,\n",
       " 'durationMs': {'addBatch': 11606,\n",
       "  'getBatch': 10,\n",
       "  'queryPlanning': 120,\n",
       "  'triggerExecution': 11828},\n",
       " 'eventTime': {'watermark': '1970-01-01T00:00:00.000Z'},\n",
       " 'stateOperators': [{'numRowsTotal': 0,\n",
       "   'numRowsUpdated': 0,\n",
       "   'memoryUsedBytes': 44599,\n",
       "   'customMetrics': {'loadedMapCacheHitCount': 0,\n",
       "    'loadedMapCacheMissCount': 0,\n",
       "    'stateOnCurrentVersionSizeBytes': 15799}}],\n",
       " 'sources': [{'description': 'KafkaV2[Subscribe[readings_raw]]',\n",
       "   'startOffset': None,\n",
       "   'endOffset': {'readings_raw': {'0': 65000}},\n",
       "   'numInputRows': 0,\n",
       "   'processedRowsPerSecond': 0.0}],\n",
       " 'sink': {'description': 'org.apache.spark.sql.kafka010.KafkaSourceProvider@1b57f220'}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(15)\n",
    "kafkaWriteQuery.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Processing new data',\n",
       " 'isDataAvailable': True,\n",
       " 'isTriggerActive': True}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafkaWriteQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0226cc16-5616-4ca7-a484-423e99021fe1',\n",
       " 'runId': '93ce24a1-a2dc-4369-869e-491ccbf0b441',\n",
       " 'name': None,\n",
       " 'timestamp': '2020-03-02T15:28:37.936Z',\n",
       " 'batchId': 1,\n",
       " 'numInputRows': 33000,\n",
       " 'inputRowsPerSecond': 2789.753994420492,\n",
       " 'processedRowsPerSecond': 2117.9641871510175,\n",
       " 'durationMs': {'addBatch': 15369,\n",
       "  'getBatch': 1,\n",
       "  'getEndOffset': 0,\n",
       "  'queryPlanning': 86,\n",
       "  'setOffsetRange': 48,\n",
       "  'triggerExecution': 15580,\n",
       "  'walCommit': 29},\n",
       " 'eventTime': {'avg': '2013-09-01T03:48:44.664Z',\n",
       "  'max': '2013-09-01T18:08:00.000Z',\n",
       "  'min': '2013-08-31T22:00:20.000Z',\n",
       "  'watermark': '1970-01-01T00:00:00.000Z'},\n",
       " 'stateOperators': [{'numRowsTotal': 33000,\n",
       "   'numRowsUpdated': 33000,\n",
       "   'memoryUsedBytes': 8204303,\n",
       "   'customMetrics': {'loadedMapCacheHitCount': 200,\n",
       "    'loadedMapCacheMissCount': 0,\n",
       "    'stateOnCurrentVersionSizeBytes': 8146703}}],\n",
       " 'sources': [{'description': 'KafkaV2[Subscribe[readings_raw]]',\n",
       "   'startOffset': {'readings_raw': {'0': 65000}},\n",
       "   'endOffset': {'readings_raw': {'0': 98000}},\n",
       "   'numInputRows': 33000,\n",
       "   'inputRowsPerSecond': 2789.753994420492,\n",
       "   'processedRowsPerSecond': 2117.9641871510175}],\n",
       " 'sink': {'description': 'org.apache.spark.sql.kafka010.KafkaSourceProvider@1b57f220'}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafkaWriteQuery.lastProgress\n",
    "# kafkaWriteQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}