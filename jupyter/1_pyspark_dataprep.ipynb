{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Data Preparation\n",
    "\n",
    "This notebook showcases the simple data preparation to be done on the raw streaming data on `readings_raw` topic using PySpark. I am using Spark's structured streaming as it is the new way to do Streaming in Spark, and offers SQL-like transformations which will ease development. However, there is one small thing that worries me when it comes to Spark and Kafka integration -- even the latest, 2.4.x, Spark version is still using the old Kafka broker 0.10 version.\n",
    "\n",
    "The topic streams a comma-separated value format, which we will need to parse, deduplicate, and drop if any of the field values are null. The resulting clean data will then streamed back to `readings_prepared` Kafka topic, to be consumed by Spark streaming jobs downstream.\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we import required libraries and define variables to be used to control the stream I/O. The Kafka Bootstrap Server lives on `kafka-m` hostname, which is resolvable in Google Cloud VPC network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaBootstrapServer = \"kafka-m:9092\"\n",
    "kafkaSourceTopic = \"readings_raw\"\n",
    "kafkaTargetTopic = \"readings_prepared\"\n",
    "checkpointLocation = \"/tmp\"\n",
    "deduplicateWindow = \"1 minute\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Stream and Prepare The Data\n",
    "\n",
    "### Connect to Kafka and Subscribe to `readings_raw` Topic\n",
    "\n",
    "Connect to Kafka and create the streaming DataFrame. We then take a look at the schema produced. In Jupyter Notebooks or PySpark shell, the `spark` variable is created by default. There is no need to create another one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaSourceDF = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer) \\\n",
    "    .option(\"subscribe\", kafkaSourceTopic) \\\n",
    "    .option(\"failOnDataLoss\", False) \\\n",
    "    .load()\n",
    "kafkaSourceDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peek at the Source Streaming Data\n",
    "\n",
    "We can peek at the Kafka source stream, `kafkaSourceDF`, to ensure that Spark is reading the stream properly. There were times when, somehow, it didn't read data from the stream although the producer is producing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaSourceQuery = kafkaSourceDF.writeStream\\\n",
    "    .queryName(\"kafka_source\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait 5 seconds before querying\n",
    "time.sleep(5);\n",
    "spark.sql(\"select * from kafka_source\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaSourceQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print last progress, if necessary, and stop the \n",
    "# If we continue the query, we will eat the Driver's memory unnecessarily.\n",
    "# kafkaSourceQuery.lastProgress\n",
    "kafkaSourceQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the Comma-separated Values and Output an Array\n",
    "\n",
    "We then use the `split()` function to split the `value` column, whose the comma separated values into a column with values in an array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the CSV value from KafkaDF and turn it into an array\n",
    "csvDF = kafkaSourceDF.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), \"data STRING\").alias(\"value\")\n",
    ").select(\n",
    "    split(col(\"value.data\"), \",\").alias(\"value\")\n",
    ")\n",
    "csvDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the Array Column and Sanitise\n",
    "\n",
    "Drop duplicates within **arbitrary** 1 minute watermark. What it means is that Spark keeps the state of the stream, then use the state to deduplicate records with `reading_ts` no later than 1 minute window backwards from the max seen `reading_ts`. The window duration is configurable, but the longer it takes, the more memory it consumes to keep the state in-memory. The `message_id` column is set to string to be able to be used as Kafka topic key.\n",
    "\n",
    "See [Spark Structured Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the CSV and drop the duplicate values\n",
    "# The stream from Kafka may deliver a message at least once\n",
    "readingsDF = csvDF\\\n",
    "    .withColumn(\"message_id\", col(\"value\").getItem(0).cast(\"string\"))\\\n",
    "    .withColumn(\"reading_ts\", col(\"value\").getItem(1).cast(\"integer\").cast(\"timestamp\"))\\\n",
    "    .withColumn(\"reading_value\", col(\"value\").getItem(2).cast(\"float\"))\\\n",
    "    .withColumn(\"reading_type\", col(\"value\").getItem(3).cast(\"integer\"))\\\n",
    "    .withColumn(\"plug_id\", col(\"value\").getItem(4).cast(\"integer\"))\\\n",
    "    .withColumn(\"household_id\", col(\"value\").getItem(5).cast(\"integer\"))\\\n",
    "    .withColumn(\"house_id\", trim(col(\"value\").getItem(6)).cast(\"integer\"))\\\n",
    "    .drop(\"value\")\\\n",
    "    .dropna()\\\n",
    "    .withWatermark(\"reading_ts\", deduplicateWindow)\\\n",
    "    .dropDuplicates()\n",
    "\n",
    "readingsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peek the Parsed and Prepared Data Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readingsQuery = readingsDF.writeStream\\\n",
    "    .queryName(\"readings\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readingsQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sleep 40 seconds because we need to wait for the streaming state to initialise\n",
    "time.sleep(40)\n",
    "spark.sql(\"select * from readings\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to stop this later\n",
    "# readingsQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Back Prepared Data to Kafka\n",
    "\n",
    "Now that the data is clean, we write back to Kafka -- to the `readings_prepared` topic to be consumed by alerting and ingestion streaming jobs downstream. Write as soon as data is available, without a time-based trigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaWriteQuery = readingsDF.selectExpr(\"timestamp AS key\", \"CAST(to_json(struct(*)) AS STRING) AS value\")\\\n",
    "    .writeStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\\\n",
    "    .option(\"checkpointLocation\", checkpointLocation)\\\n",
    "    .option(\"topic\", kafkaTargetTopic)\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the status and last progress of the write to Kafka using `.lastProgress` and `status`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(15)\n",
    "kafkaWriteQuery.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaWriteQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaWriteQuery.lastProgress\n",
    "# kafkaWriteQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Back Prepared Data to Kafka\n",
    "\n",
    "Now that the data is clean, we write back to Kafka -- to the `readings_prepared` topic to be consumed by alerting and ingestion streaming jobs downstream. Write as soon as data is available, without a time-based trigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}