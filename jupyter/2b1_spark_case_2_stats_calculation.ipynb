{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2 Part 1: Calculating Running Average and Standard Deviation\n",
    "\n",
    "The 2nd of the analytics use case to be tackled is another real-time alerting system:\n",
    "\n",
    "`Hourly consumption for a household is higher than 1 standard deviation of mean consumption across all households within that particular hour on that day.`\n",
    "\n",
    "The use case is split into multiple parts, like the first use case. This notebook showcases the first part.\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "Import all the required libraries and set the stream configuration variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://spark-alert-2-stats-m:8088/proxy/application_1583161279001_0002\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = yarn, app id = application_1583161279001_0002)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "import java.sql.Timestamp\n",
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode, GroupState}\n",
       "import org.apache.spark.sql.streaming.Trigger\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "import java.sql.Timestamp\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode, GroupState}\n",
    "import org.apache.spark.sql.streaming.Trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kafkaBootstrapServer: String = kafka-m:9092\n",
       "kafkaSourceTopic: String = readings_prepared\n",
       "kafkaTargetTopic: String = alert_2_stats\n",
       "checkpointLocation: String = /tmp\n",
       "triggerTime: String = 1 minute\n",
       "deduplicateWindow: String = 1 minute\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kafkaBootstrapServer = \"kafka-m:9092\"\n",
    "val kafkaSourceTopic = \"readings_prepared\"\n",
    "val kafkaTargetTopic = \"alert_2_stats\"\n",
    "val checkpointLocation = \"/tmp\"\n",
    "val triggerTime = \"1 minute\"\n",
    "val deduplicateWindow = \"1 minute\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define The Required Schema and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mySchema: org.apache.spark.sql.types.StructType = StructType(StructField(message_id,StringType,false), StructField(reading_ts,TimestampType,false), StructField(reading_value,FloatType,false), StructField(reading_type,IntegerType,false), StructField(plug_id,IntegerType,false), StructField(household_id,IntegerType,false), StructField(house_id,IntegerType,false))\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// This will be used to give the source `readings_prepared` stream data a schema\n",
    "val mySchema = StructType(Seq(\n",
    "    StructField(\"message_id\", StringType, false),\n",
    "    StructField(\"reading_ts\", TimestampType, false),\n",
    "    StructField(\"reading_value\", FloatType, false),\n",
    "    StructField(\"reading_type\", IntegerType, false),\n",
    "    StructField(\"plug_id\", IntegerType, false),\n",
    "    StructField(\"household_id\", IntegerType, false),\n",
    "    StructField(\"house_id\", IntegerType, false)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class ReadingsInput\n",
       "defined class StatsState\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Somehow case classes need to be defined in a separate cell\n",
    "// This will be used in the stateful streaming calculation as the input rows\n",
    "case class ReadingsInput(\n",
    "    day: String, \n",
    "    hour: Int, \n",
    "    reading_value: Float,\n",
    "    reading_ts: java.sql.Timestamp \n",
    ")\n",
    "\n",
    "// This will be used in the stateful streaming calculation as the state store schema\n",
    "case class StatsState(\n",
    "    day: String,\n",
    "    hour: Int,\n",
    "    mean: Float,\n",
    "    m2: Float,\n",
    "    variance: Float,\n",
    "    std_dev: Float,\n",
    "    count: Long, \n",
    "    last_ts: java.sql.Timestamp // Will be used in stream-to-stream join\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and Parse The Input Stream\n",
    "\n",
    "Only take the Current Load readings (`reading_type = 1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readings: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [message_id: string, reading_ts: timestamp ... 5 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val readings = spark\n",
    "    .readStream \n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n",
    "    .option(\"subscribe\", kafkaSourceTopic)\n",
    "    .option(\"failOnDataLoss\", false)\n",
    "    .load()\n",
    "    .selectExpr(\"CAST(value AS STRING)\")\n",
    "    .select(from_json($\"value\", mySchema).as(\"data\"))\n",
    "    .select($\"data.*\")\n",
    "    .withWatermark(\"reading_ts\", deduplicateWindow) \n",
    "    .dropDuplicates()\n",
    "    .filter($\"reading_type\" === 1) // Only take the \"current load\" measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peek at The Input Data Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "streamQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@651e97e2\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val streamQuery = readings.writeStream.format(\"memory\").queryName(\"readings\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Getting offsets from KafkaV2[Subscribe[readings_prepared]]\",\n",
       "  \"isDataAvailable\" : false,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------+------------+-------+------------+--------+\n",
      "|message_id|         reading_ts|reading_value|reading_type|plug_id|household_id|house_id|\n",
      "+----------+-------------------+-------------+------------+-------+------------+--------+\n",
      "| 118504114|2013-09-01 17:53:00|          0.0|           1|      0|           0|       9|\n",
      "| 115998526|2013-09-01 17:27:20|          0.0|           1|      0|           0|       0|\n",
      "| 116872749|2013-09-01 17:36:20|          0.0|           1|      1|           0|       9|\n",
      "| 118371375|2013-09-01 17:51:40|          0.0|           1|      2|           0|       4|\n",
      "| 117153098|2013-09-01 17:39:20|          0.0|           1|      0|           0|       5|\n",
      "| 117306922|2013-09-01 17:41:00|       50.627|           1|      0|           0|       8|\n",
      "| 118503302|2013-09-01 17:53:00|       41.056|           1|      2|           0|       0|\n",
      "| 118868629|2013-09-01 17:56:40|      122.738|           1|      1|           0|       1|\n",
      "| 119725890|2013-09-01 18:05:20|          0.0|           1|      0|           0|       6|\n",
      "| 119590206|2013-09-01 18:04:00|          0.0|           1|      2|           0|       7|\n",
      "| 119496230|2013-09-01 18:03:00|          0.0|           1|      1|           0|       6|\n",
      "| 119465366|2013-09-01 18:02:40|          0.0|           1|      2|           0|       9|\n",
      "| 119136988|2013-09-01 17:59:20|          0.0|           1|      0|           0|       4|\n",
      "| 119333968|2013-09-01 18:01:20|        40.97|           1|      2|           0|       0|\n",
      "| 119300642|2013-09-01 18:01:00|          0.0|           1|      2|           0|       2|\n",
      "| 119336636|2013-09-01 18:01:20|          0.0|           1|      0|           0|       9|\n",
      "| 119234296|2013-09-01 18:00:20|      140.243|           1|      1|           0|       1|\n",
      "| 119689344|2013-09-01 18:05:00|          0.0|           1|      1|           0|       3|\n",
      "| 119465250|2013-09-01 18:02:40|          0.0|           1|      0|           0|       5|\n",
      "| 119590291|2013-09-01 18:04:00|       62.252|           1|      0|           0|       8|\n",
      "+----------+-------------------+-------------+------------+-------+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Thread.sleep(60000)\n",
    "spark.sql(\"select * from readings\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: org.apache.spark.sql.streaming.StreamingQueryProgress =\n",
       "{\n",
       "  \"id\" : \"4cd512af-ec55-4a22-a1a0-ff5f8f9d1514\",\n",
       "  \"runId\" : \"d9f1ef0f-335a-41d8-85c9-18087163e7c6\",\n",
       "  \"name\" : \"readings\",\n",
       "  \"timestamp\" : \"2020-03-02T15:34:51.144Z\",\n",
       "  \"batchId\" : 14,\n",
       "  \"numInputRows\" : 5149,\n",
       "  \"inputRowsPerSecond\" : 177.05718510367595,\n",
       "  \"processedRowsPerSecond\" : 511.11772880682946,\n",
       "  \"durationMs\" : {\n",
       "    \"addBatch\" : 9909,\n",
       "    \"getBatch\" : 0,\n",
       "    \"getEndOffset\" : 0,\n",
       "    \"queryPlanning\" : 62,\n",
       "    \"setOffsetRange\" : 3,\n",
       "    \"triggerExecution\" : 10074,\n",
       "    \"walCommit\" : 34\n",
       "  },\n",
       "  \"eventTime\" : {\n",
       "    \"avg\" : \"2013-09-02T03:16:41.204Z\",\n",
       "    \"max\" : \"2013-09-02T04:04:20.000Z\",\n",
       "    \"min\" : \"2013-09-02T02:19:20.000Z\",\n",
       "    \"watermark\" : \"2013-09-02T02:35:20.000Z\"\n",
       "  },\n",
       "  \"stateOperators\" : [ {\n",
       "    \"numRowsTotal\" ..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// streamQuery.stop()\n",
    "streamQuery.lastProgress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate The Running Average and Standard Deviation\n",
    "\n",
    "To complete the first task, one thing to keep in mind is that we are calculating a running/moving average/standard deviation over an unbounded stream of data. With that in mind, the typical algorithm of calculating average over a limited amount of bounded/batch data, `average = sum_of_values / population_size`, won't work. \n",
    "\n",
    "Why? Since we need to sum all values on a stream, we need to store the state of the sum at every given point in time, and as the stream goes on and one, this can lead to numeric overflow issues. Instead of doing that, we can incrementally calculate average by keeping track of the current average and the current population size / record count, and then adjust as new data comes.  We can do that using [Welford's online algorithm](https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_online_algorithm).\n",
    "\n",
    "#### Storing The Running Statistics\n",
    "\n",
    "Once we have a stream of running average and standard deviation, ideally, I want to output and store the stats in a dataset whose very low latency lookup speed like HBase or Google BigTable. However, due to technical issues with HBase's Spark library, I store the data back to Kafka on the `alert_1_stats` stream. I did not want to spend too much time trying to resolve the issue so I decided to drop it.\n",
    "\n",
    "This [blog post](https://medium.com/@robbinjain19/challenges-faced-while-integrating-apache-spark-with-hbase-and-the-solution-5c1c8a068808) suggests that I downgrade to Spark 2.1.0 to resolve the HBase connector issue, but I need to use the `foreachBatch` functionality from Spark 2.4.0 and stream-to-stream join from Spark 2.3.0.\n",
    "\n",
    "\n",
    "### Define the Functions to Be Used in MapGroupsWithState\n",
    "\n",
    "I created 2 functions: `updateHouseHourStats(state, input)` and `updateAllHouseHourStats(key, input, groupState)`. The latter will be called by Spark's `mapGroupsWithState` function and call the former to work on individual rows to update the state.\n",
    "\n",
    "The case states \"`across all households within that particular hour on that day.`\". I am assuming that `that day` means a particular combinarion of day-month-year. Hence here, the grouping keys used are `date_format(reading_ts, 'yyyyMMdd')` and the `hour(reading_ts)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateHouseHourStats: (state: StatsState, input: ReadingsInput)StatsState\n",
       "updateAllHouseHourStats: (key: (String, Int), inputs: Iterator[ReadingsInput], groupState: org.apache.spark.sql.streaming.GroupState[StatsState])StatsState\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/**\n",
    " * To be called by updateAllHouseHourStats\n",
    " */\n",
    "def updateHouseHourStats(state: StatsState, input: ReadingsInput): StatsState = {\n",
    "    // This is implementing Welford's moving average / standard deviation algorithm\n",
    "    val newCount = state.count + 1\n",
    "    val delta = input.reading_value - state.mean\n",
    "    val newMean = state.mean + (delta / newCount)\n",
    "    val newDelta = input.reading_value - newMean\n",
    "    val newM2 = state.m2 + (delta * newDelta)\n",
    "    // Calculate Sample variance, state.count == newCount - 1\n",
    "    val newVariance = if(newCount > 1) { newM2 / state.count } else { 0 } \n",
    "    val newStdDev = if(newCount > 1) { Math.sqrt(newVariance).toFloat } else { 0 }\n",
    "\n",
    "    return StatsState(\n",
    "        state.day,\n",
    "        state.hour,\n",
    "        newMean,\n",
    "        newM2,\n",
    "        newVariance,\n",
    "        newStdDev,\n",
    "        newCount,\n",
    "        input.reading_ts\n",
    "    )\n",
    "}\n",
    "\n",
    "/**\n",
    " * To be called by mapGroupWithState\n",
    " */\n",
    "def updateAllHouseHourStats(\n",
    "  key: (String, Int),\n",
    "  inputs: Iterator[ReadingsInput],\n",
    "  groupState: GroupState[StatsState]\n",
    ") : StatsState = {\n",
    "    // Get previous state if exists, else create a new empty state\n",
    "    var currentStatsState = groupState.getOption.getOrElse {\n",
    "        new StatsState(\n",
    "            key._1, // Day of week\n",
    "            key._2, // Hour\n",
    "            0,  // Mean\n",
    "            0,  // m2\n",
    "            0,  // Variance\n",
    "            0,  // STD Dev\n",
    "            0,   // Count \n",
    "            null\n",
    "        )\n",
    "    }\n",
    "    // Loop over the inputs in this microbatch\n",
    "    for (input <- inputs) {\n",
    "        currentStatsState = updateHouseHourStats(currentStatsState, input)\n",
    "    }\n",
    "    // Update the current state\n",
    "    groupState.update(currentStatsState)\n",
    "    // Return the current state to the stream\n",
    "    return currentStatsState\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send Update to Kafka\n",
    "\n",
    "Pull the trigger every arbitrary **1 minute**, as set in `triggerTime`, since we may want to wait until we accumulate enough data for statistics calculation. We are using the `GroupStateTimeout.NoTimeout` option -- meaning that the stats state will continually gets updated until the stream is stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stats: org.apache.spark.sql.Dataset[StatsState] = [day: string, hour: int ... 6 more fields]\n",
       "query: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@4f6ae761\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stats = readings\n",
    "    .select(\n",
    "        date_format($\"reading_ts\", \"yyyyMMdd\").alias(\"day\"),\n",
    "        hour($\"reading_ts\").alias(\"hour\"), \n",
    "        $\"reading_value\",\n",
    "        $\"reading_ts\"\n",
    "    )\n",
    "    .toDF()\n",
    "    .as[ReadingsInput]\n",
    "    .groupByKey(\n",
    "        input => (input.day, input.hour)\n",
    "    )\n",
    "    .mapGroupsWithState(GroupStateTimeout.NoTimeout)(updateAllHouseHourStats)\n",
    "    \n",
    "val query = stats\n",
    "    .selectExpr(\"CONCAT(day, '-', hour, '-', last_ts) AS key\", \"CAST(to_json(struct(*)) AS STRING) AS value\")\n",
    "    .writeStream\n",
    "    .outputMode(\"update\")\n",
    "    .format(\"kafka\")\n",
    "    .trigger(Trigger.ProcessingTime(triggerTime))\n",
    "    .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n",
    "    .option(\"checkpointLocation\", checkpointLocation)\n",
    "    .option(\"topic\", kafkaTargetTopic)\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Processing new data\",\n",
       "  \"isDataAvailable\" : true,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Thread.sleep(10000)\n",
    "query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: org.apache.spark.sql.streaming.StreamingQueryProgress =\n",
       "{\n",
       "  \"id\" : \"06cc3b45-c2f4-4e5a-b018-44f182391393\",\n",
       "  \"runId\" : \"3a1614d4-5eea-47be-bd80-49fdd6cda09c\",\n",
       "  \"name\" : null,\n",
       "  \"timestamp\" : \"2020-03-02T15:36:00.000Z\",\n",
       "  \"batchId\" : 4,\n",
       "  \"numInputRows\" : 11468,\n",
       "  \"inputRowsPerSecond\" : 191.13333333333333,\n",
       "  \"processedRowsPerSecond\" : 552.8612061900401,\n",
       "  \"durationMs\" : {\n",
       "    \"addBatch\" : 20488,\n",
       "    \"getBatch\" : 0,\n",
       "    \"getEndOffset\" : 1,\n",
       "    \"queryPlanning\" : 150,\n",
       "    \"setOffsetRange\" : 1,\n",
       "    \"triggerExecution\" : 20743,\n",
       "    \"walCommit\" : 34\n",
       "  },\n",
       "  \"eventTime\" : {\n",
       "    \"avg\" : \"2013-09-02T06:31:45.667Z\",\n",
       "    \"max\" : \"2013-09-02T08:28:00.000Z\",\n",
       "    \"min\" : \"2013-09-02T04:40:40.000Z\",\n",
       "    \"watermark\" : \"2013-09-02T04:57:00.000Z\"\n",
       "  },\n",
       "  \"stateOperators\" : [ {\n",
       "    \"numRowsTotal\" : 12..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// query.stop()\n",
    "query.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}