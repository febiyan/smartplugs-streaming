{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1 Part 2: Anomaly Detection\n",
    "\n",
    "This notebook showcases the 2nd part of the analytics use case to be tackled in a real-time alerting system:\n",
    "\n",
    "`Hourly consumption for a household is higher than 1 standard deviation of that household's historical mean consumption for that hour.`\n",
    "\n",
    "The second part here is to read the data stream from both `readings_prepared` and `alert_1_stats` and use them to detect data anomalies -- the ones that go above 1 standard deviation from the mean. The detected anomalies are then stored in a persistent data store, which in this case is Google BigQuery.\n",
    "\n",
    "BigQuery is chosen for the fit of further analytical queries down the line. We might be interested to do some BI or advanced analysis down the line. It is also serverless -- we just need to define the Datasets and Tables.\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "Import all the required libraries and set the stream configuration variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://spark-alert-1-detect-m:8088/proxy/application_1583313185168_0001\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = yarn, app id = application_1583313185168_0001)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.streaming.Trigger\n",
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kafkaBootstrapServer: String = kafka-m:9092\n",
       "kafkaReadingsTopic: String = readings_prepared\n",
       "kafkaStatsTopic: String = alert_1_stats\n",
       "kafkaDedupWatermarkTime: String = 1 minute\n",
       "joinWatermarkTime: String = 1 minute\n",
       "bigQueryTargetTable: String = smartplugs.alert_1_anomaly\n",
       "bigQueryTempBucket: String = pandora-sde-case/alert_1\n",
       "outputTriggerTime: String = 1 minute\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kafkaBootstrapServer = \"kafka-m:9092\"\n",
    "val kafkaReadingsTopic = \"readings_prepared\"\n",
    "val kafkaStatsTopic = \"alert_1_stats\"\n",
    "val kafkaDedupWatermarkTime = \"1 minute\"\n",
    "val joinWatermarkTime = \"1 minute\"\n",
    "val bigQueryTargetTable = \"smartplugs.alert_1_anomaly\"\n",
    "val bigQueryTempBucket = \"pandora-sde-case/alert_1\"\n",
    "val outputTriggerTime = \"1 minute\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define The Required Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readingsSchema: org.apache.spark.sql.types.StructType = StructType(StructField(message_id,StringType,false), StructField(reading_ts,TimestampType,false), StructField(reading_value,FloatType,false), StructField(reading_type,IntegerType,false), StructField(plug_id,IntegerType,false), StructField(household_id,IntegerType,false), StructField(house_id,IntegerType,false))\n",
       "statsSchema: org.apache.spark.sql.types.StructType = StructType(StructField(house_id,IntegerType,false), StructField(hour,IntegerType,false), StructField(mean,FloatType,false), StructField(m2,FloatType,false), StructField(variance,FloatType,false), StructField(std_dev,FloatType,false), StructField(count,LongType,false), StructField(last_ts,TimestampType,false))\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// This will be used to give the source `readings_prepared` stream data a schema\n",
    "val readingsSchema = StructType(Seq(\n",
    "    StructField(\"message_id\", StringType, false),\n",
    "    StructField(\"reading_ts\", TimestampType, false),\n",
    "    StructField(\"reading_value\", FloatType, false),\n",
    "    StructField(\"reading_type\", IntegerType, false),\n",
    "    StructField(\"plug_id\", IntegerType, false),\n",
    "    StructField(\"household_id\", IntegerType, false),\n",
    "    StructField(\"house_id\", IntegerType, false)\n",
    "))\n",
    "\n",
    "val statsSchema = StructType(Seq(\n",
    "    StructField(\"house_id\", IntegerType, false),\n",
    "    StructField(\"hour\", IntegerType, false),\n",
    "    StructField(\"mean\", FloatType, false),\n",
    "    StructField(\"m2\", FloatType, false),\n",
    "    StructField(\"variance\", FloatType, false),\n",
    "    StructField(\"std_dev\", FloatType, false),\n",
    "    StructField(\"count\", LongType, false),\n",
    "    StructField(\"last_ts\", TimestampType, false)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and Parse The Input Data Streams\n",
    "\n",
    "There are 2 input streams this time, `readings_prep` and `alert_1_stats` topic. They will be joined to detect anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readings: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [message_id: string, reading_ts: timestamp ... 5 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Drop duplicates if seen in an arbitrary watermark. Bounds are necessary so that Spark does not store \n",
    "// ALL records in the state\n",
    "val readings = spark\n",
    "    .readStream \n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n",
    "    .option(\"subscribe\", kafkaReadingsTopic)\n",
    "    .load()\n",
    "    .selectExpr(\"CAST(value AS STRING)\")\n",
    "    .select(from_json($\"value\", readingsSchema).as(\"data\"))\n",
    "    .select($\"data.*\")\n",
    "    .withWatermark(\"reading_ts\", kafkaDedupWatermarkTime) \n",
    "    .dropDuplicates()\n",
    "    .filter($\"reading_type\" === 1) // Only take the \"current load\" measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stats: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [house_id: int, hour: int ... 3 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// We don't need to deduplicate the stats, but we drop the ones with mean=0\n",
    "val stats = spark\n",
    "    .readStream \n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n",
    "    .option(\"subscribe\", kafkaStatsTopic)\n",
    "    .load()\n",
    "    .selectExpr(\"CAST(value AS STRING)\")\n",
    "    .select(from_json($\"value\", statsSchema).as(\"data\"))\n",
    "    .select($\"data.house_id\", $\"data.hour\", $\"data.mean\", $\"data.std_dev\", $\"data.last_ts\")\n",
    "    .filter($\"mean\" > 0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peek at The Input Data Streams\n",
    "\n",
    "##### Readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readingsQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@3e63118e\n",
       "res0: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Processing new data\",\n",
       "  \"isDataAvailable\" : true,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val readingsQuery = readings.writeStream.format(\"memory\").queryName(\"readings\").start()\n",
    "Thread.sleep(10000)\n",
    "readingsQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------+------------+-------+------------+--------+\n",
      "|message_id|reading_ts|reading_value|reading_type|plug_id|household_id|house_id|\n",
      "+----------+----------+-------------+------------+-------+------------+--------+\n",
      "+----------+----------+-------------+------------+-------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"select * from readings\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: org.apache.spark.sql.streaming.StreamingQueryProgress =\n",
       "{\n",
       "  \"id\" : \"4d7065fb-beda-47bc-8762-23343f78cfd2\",\n",
       "  \"runId\" : \"b19775d4-c45a-4f23-acd0-842b2c6f8a07\",\n",
       "  \"name\" : \"readings\",\n",
       "  \"timestamp\" : \"2020-03-04T09:19:31.369Z\",\n",
       "  \"batchId\" : 1,\n",
       "  \"numInputRows\" : 0,\n",
       "  \"inputRowsPerSecond\" : 0.0,\n",
       "  \"processedRowsPerSecond\" : 0.0,\n",
       "  \"durationMs\" : {\n",
       "    \"getEndOffset\" : 1,\n",
       "    \"setOffsetRange\" : 51,\n",
       "    \"triggerExecution\" : 63\n",
       "  },\n",
       "  \"eventTime\" : {\n",
       "    \"watermark\" : \"1970-01-01T00:00:00.000Z\"\n",
       "  },\n",
       "  \"stateOperators\" : [ {\n",
       "    \"numRowsTotal\" : 0,\n",
       "    \"numRowsUpdated\" : 0,\n",
       "    \"memoryUsedBytes\" : 44599,\n",
       "    \"customMetrics\" : {\n",
       "      \"loadedMapCacheHitCount\" : 0,\n",
       "      \"loadedMapCacheMissCount\" : 0,\n",
       "      \"stateOnCurrentVersionSizeBytes\" : 15799\n",
       "    }\n",
       "  } ],\n",
       "  \"sources\" : [ {\n",
       "    \"descr..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// readingsQuery.stop()\n",
    "readingsQuery.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Getting offsets from KafkaV2[Subscribe[readings_prepared]]\",\n",
       "  \"isDataAvailable\" : false,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readingsQuery.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "statsQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@7ab28881\n",
       "res4: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Getting offsets from KafkaV2[Subscribe[alert_1_stats]]\",\n",
       "  \"isDataAvailable\" : false,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val statsQuery = stats.writeStream.format(\"memory\").queryName(\"stats\").start()\n",
    "statsQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+-------+-------+\n",
      "|house_id|hour|mean|std_dev|last_ts|\n",
      "+--------+----+----+-------+-------+\n",
      "+--------+----+----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Thread.sleep(10000)\n",
    "spark.sql(\"select * from stats\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: org.apache.spark.sql.streaming.StreamingQueryProgress =\n",
       "{\n",
       "  \"id\" : \"ecd4dd6c-94a7-49c0-b50f-068c000459c0\",\n",
       "  \"runId\" : \"bec3a72c-1e52-423f-8dc7-78dded1a5bb4\",\n",
       "  \"name\" : \"stats\",\n",
       "  \"timestamp\" : \"2020-03-04T09:19:34.075Z\",\n",
       "  \"batchId\" : 1,\n",
       "  \"numInputRows\" : 0,\n",
       "  \"inputRowsPerSecond\" : 0.0,\n",
       "  \"processedRowsPerSecond\" : 0.0,\n",
       "  \"durationMs\" : {\n",
       "    \"getEndOffset\" : 0,\n",
       "    \"setOffsetRange\" : 2,\n",
       "    \"triggerExecution\" : 2\n",
       "  },\n",
       "  \"stateOperators\" : [ ],\n",
       "  \"sources\" : [ {\n",
       "    \"description\" : \"KafkaV2[Subscribe[alert_1_stats]]\",\n",
       "    \"startOffset\" : {\n",
       "      \"alert_1_stats\" : {\n",
       "        \"0\" : 0\n",
       "      }\n",
       "    },\n",
       "    \"endOffset\" : {\n",
       "      \"alert_1_stats\" : {\n",
       "        \"0\" : 0\n",
       "      }\n",
       "    },\n",
       "    \"numInputRows\" : 0,\n",
       "    \"inputRowsPerSecond\" : 0.0,\n",
       "    \"processedRowsPerSecond\" : 0.0\n",
       "  } ],\n",
       "  \"sink\" ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// statsQuery.stop()\n",
    "statsQuery.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Waiting for data to arrive\",\n",
       "  \"isDataAvailable\" : false,\n",
       "  \"isTriggerActive\" : false\n",
       "}\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statsQuery.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Both Data Streams\n",
    "\n",
    "\n",
    "We can only compare the readings with the past statistics. The `reading_ts` value of `readings_prepared` stream needs to be bigger than the `last_ts` value of the `alert_1_stats` stream. We need to ensure we are comparing the stats from the right `house_id` and `hour`.\n",
    "\n",
    "After joining, we need to ensure that we only compare the readings with their latest past statistics. The `alert_1_stats` stream is assumed to be slower to arrive compared to the readings_prepared stream, since it needs to wait at least 2 records to start calculating, and the default write trigger is every 1 minute. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "anomaly: org.apache.spark.sql.DataFrame = [message_id: string, reading_ts: timestamp ... 9 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Anomaly detection is done by getting the latest std_dev and mean value\n",
    "// Then act on it by comparing to the \n",
    "val anomaly = readings\n",
    "    .withColumnRenamed(\"house_id\", \"readings_house_id\")\n",
    "    .withWatermark(\"reading_ts\", joinWatermarkTime)\n",
    "    .join(\n",
    "        stats.withWatermark(\"last_ts\", joinWatermarkTime),\n",
    "        // Join conditions\n",
    "        $\"reading_ts\" > $\"last_ts\" &&\n",
    "        hour($\"reading_ts\") === $\"hour\" &&\n",
    "        $\"readings_house_id\" === $\"house_id\",\n",
    "        // Join type\n",
    "        \"inner\"\n",
    "    )\n",
    "    .filter($\"reading_value\" > $\"mean\" + $\"std_dev\")\n",
    "    .drop(\"readings_house_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peekQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@5f3351a2\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val peekQuery = anomaly\n",
    "    .writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"anomaly\")\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from anomaly\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: org.apache.spark.sql.streaming.StreamingQueryProgress = null\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peekQuery.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Processing new data\",\n",
       "  \"isDataAvailable\" : true,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peekQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "// peekQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to BigQuery\n",
    "\n",
    "Write to BigQuery once a minute. BigQuery inserts work better with mini-batched data and this ensures that we give enough time for `alert_1_stats` to accumulate. The [BigQuery connector](https://github.com/GoogleCloudDataproc/spark-bigquery-connector) is installed during cluster setup and loaded automatically when spark-shell session is initiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ingestQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@79a1fc09\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ingestQuery = anomaly\n",
    "    .writeStream\n",
    "    .trigger(Trigger.ProcessingTime(outputTriggerTime))\n",
    "    .foreachBatch{ \n",
    "        (batchDF: DataFrame, batchId: Long) =>\n",
    "            batchDF.write.format(\"bigquery\")\n",
    "                .option(\"table\", bigQueryTargetTable)\n",
    "                .option(\"temporaryGcsBucket\", bigQueryTempBucket)\n",
    "                .mode(SaveMode.Append)\n",
    "                .save()\n",
    "    }.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestQuery.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "//ingestQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}