{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1 Part 2: Anomaly Detection\n",
    "\n",
    "This notebook showcases the 2nd part of the analytics use case to be tackled in a real-time alerting system:\n",
    "\n",
    "`Hourly consumption for a household is higher than 1 standard deviation of that household's historical mean consumption for that hour.`\n",
    "\n",
    "The second part here is to read the data stream from both `readings_prepared` and `alert_1_stats` and use them to detect data anomalies -- the ones that go above 1 standard deviation from the mean. The detected anomalies are then stored in a persistent data store, which in this case is Google BigQuery.\n",
    "\n",
    "BigQuery is chosen for the fit of further analytical queries down the line. We might be interested to do some BI or advanced analysis down the line. It is also serverless -- we just need to define the Datasets and Tables.\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "Import all the required libraries and set the stream configuration variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://spark-alert-1-detect-m:8088/proxy/application_1583148924695_0001\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = yarn, app id = application_1583148924695_0001)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.streaming.Trigger\n",
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kafkaBootstrapServer: String = kafka-m:9092\n",
       "kafkaReadingsTopic: String = readings_prepared\n",
       "kafkaStatsTopic: String = alert_1_stats\n",
       "kafkaDedupWatermarkTime: String = 1 minute\n",
       "joinWatermarkTime: String = 1 minute\n",
       "bigQueryTargetTable: String = smartplugs.alert_1_anomaly\n",
       "bigQueryTempBucket: String = pandora-sde-case\n",
       "outputTriggerTime: String = 1 minute\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kafkaBootstrapServer = \"kafka-m:9092\"\n",
    "val kafkaReadingsTopic = \"readings_prepared\"\n",
    "val kafkaStatsTopic = \"alert_1_stats\"\n",
    "val kafkaDedupWatermarkTime = \"1 minute\"\n",
    "val joinWatermarkTime = \"1 minute\"\n",
    "val bigQueryTargetTable = \"smartplugs.alert_1_anomaly\"\n",
    "val bigQueryTempBucket = \"pandora-sde-case/alert_1\"\n",
    "val outputTriggerTime = \"1 minute\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define The Required Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readingsSchema: org.apache.spark.sql.types.StructType = StructType(StructField(message_id,StringType,false), StructField(reading_ts,TimestampType,false), StructField(reading_value,FloatType,false), StructField(reading_type,IntegerType,false), StructField(plug_id,IntegerType,false), StructField(household_id,IntegerType,false), StructField(house_id,IntegerType,false))\n",
       "statsSchema: org.apache.spark.sql.types.StructType = StructType(StructField(house_id,IntegerType,false), StructField(hour,IntegerType,false), StructField(mean,FloatType,false), StructField(m2,FloatType,false), StructField(variance,FloatType,false), StructField(std_dev,FloatType,false), StructField(count,LongType,false), StructField(last_ts,TimestampType,false))\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// This will be used to give the source `readings_prepared` stream data a schema\n",
    "val readingsSchema = StructType(Seq(\n",
    "    StructField(\"message_id\", StringType, false),\n",
    "    StructField(\"reading_ts\", TimestampType, false),\n",
    "    StructField(\"reading_value\", FloatType, false),\n",
    "    StructField(\"reading_type\", IntegerType, false),\n",
    "    StructField(\"plug_id\", IntegerType, false),\n",
    "    StructField(\"household_id\", IntegerType, false),\n",
    "    StructField(\"house_id\", IntegerType, false)\n",
    "))\n",
    "\n",
    "val statsSchema = StructType(Seq(\n",
    "    StructField(\"house_id\", IntegerType, false),\n",
    "    StructField(\"hour\", IntegerType, false),\n",
    "    StructField(\"mean\", FloatType, false),\n",
    "    StructField(\"m2\", FloatType, false),\n",
    "    StructField(\"variance\", FloatType, false),\n",
    "    StructField(\"std_dev\", FloatType, false),\n",
    "    StructField(\"count\", LongType, false),\n",
    "    StructField(\"last_ts\", TimestampType, false)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and Parse The Input Data Streams\n",
    "\n",
    "There are 2 input streams this time, `readings_prep` and `alert_1_stats` topic. They will be joined to detect anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Drop duplicates if seen in an arbitrary watermark. Bounds are necessary so that Spark does not store \n",
    "// ALL records in the state\n",
    "val readings = spark\n",
    "    .readStream \n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n",
    "    .option(\"subscribe\", kafkaReadingsTopic)\n",
    "    .load()\n",
    "    .selectExpr(\"CAST(value AS STRING)\")\n",
    "    .select(from_json($\"value\", readingsSchema).as(\"data\"))\n",
    "    .select($\"data.*\")\n",
    "    .withWatermark(\"reading_ts\", kafkaDedupWatermarkTime) \n",
    "    .dropDuplicates()\n",
    "    .filter($\"reading_type\" === 1) // Only take the \"current load\" measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stats: org.apache.spark.sql.DataFrame = [house_id: int, hour: int ... 6 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// We don't need to deduplicate the stats, but we drop the ones with mean=0\n",
    "val stats = spark\n",
    "    .readStream \n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n",
    "    .option(\"subscribe\", kafkaStatsTopic)\n",
    "    .load()\n",
    "    .selectExpr(\"CAST(value AS STRING)\")\n",
    "    .select(from_json($\"value\", statsSchema).as(\"data\"))\n",
    "    .select($\"data.house_id\", $\"data.hour\", $\"data.mean\", $\"data.std_dev\", $\"data.last_ts\")\n",
    "    .filter($\"mean\" > 0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peek at The Input Data Streams\n",
    "\n",
    "##### Readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readingsQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@7e4fdee9\n",
       "res0: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Getting offsets from KafkaV2[Subscribe[readings_prepared]]\",\n",
       "  \"isDataAvailable\" : false,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val readingsQuery = readings.writeStream.format(\"memory\").queryName(\"readings\").start()\n",
    "Thread.sleep(10000)\n",
    "readingsQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------+------------+-------+------------+--------+\n",
      "|message_id|         reading_ts|reading_value|reading_type|plug_id|household_id|house_id|\n",
      "+----------+-------------------+-------------+------------+-------+------------+--------+\n",
      "|  21267584|2013-09-01 01:35:00|       35.099|           0|      1|           0|       1|\n",
      "|  21371713|2013-09-01 01:36:00|       11.757|           0|      2|           0|       0|\n",
      "|  22405161|2013-09-01 01:46:20|        0.355|           0|      1|           0|       4|\n",
      "|  22471510|2013-09-01 01:47:00|          0.0|           1|      0|           0|       4|\n",
      "|  22636827|2013-09-01 01:48:40|          0.0|           1|      1|           0|       3|\n",
      "|  23903171|2013-09-01 02:01:20|        0.161|           0|      0|           0|       9|\n",
      "|  24134854|2013-09-01 02:03:40|        9.853|           1|      2|           0|       0|\n",
      "|  24302832|2013-09-01 02:05:20|         2.25|           0|      1|           0|       7|\n",
      "|  24435084|2013-09-01 02:06:40|          0.0|           1|      0|           0|       2|\n",
      "|  25134972|2013-09-01 02:13:40|          0.0|           1|      0|           0|       5|\n",
      "|  25268752|2013-09-01 02:15:00|          0.0|           1|      2|           0|       5|\n",
      "|  25599296|2013-09-01 02:18:20|        3.355|           1|      1|           0|       1|\n",
      "|  25601331|2013-09-01 02:18:20|         2.25|           0|      1|           0|       5|\n",
      "|  25832881|2013-09-01 02:20:40|        0.161|           0|      2|           0|       2|\n",
      "|  26399731|2013-09-01 02:26:20|        0.982|           0|      2|           0|       4|\n",
      "|  26566167|2013-09-01 02:28:00|          0.0|           1|      1|           0|       3|\n",
      "|  26699760|2013-09-01 02:29:20|          0.0|           1|      2|           0|       9|\n",
      "|  26932542|2013-09-01 02:31:40|        0.982|           0|      0|           0|       7|\n",
      "|  27431839|2013-09-01 02:36:40|         2.25|           0|      1|           0|       7|\n",
      "|  27631410|2013-09-01 02:38:40|          0.0|           1|      2|           0|       9|\n",
      "+----------+-------------------+-------------+------------+-------+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"select * from readings\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: org.apache.spark.sql.streaming.StreamingQueryProgress =\n",
       "{\n",
       "  \"id\" : \"9a99c6a0-e13f-4654-824c-d8fd89919a13\",\n",
       "  \"runId\" : \"9b858307-236e-4829-8378-b927c61ba527\",\n",
       "  \"name\" : \"readings\",\n",
       "  \"timestamp\" : \"2020-03-02T12:48:06.186Z\",\n",
       "  \"batchId\" : 0,\n",
       "  \"numInputRows\" : 33,\n",
       "  \"processedRowsPerSecond\" : 0.9723613648417703,\n",
       "  \"durationMs\" : {\n",
       "    \"addBatch\" : 30895,\n",
       "    \"getBatch\" : 11,\n",
       "    \"getEndOffset\" : 0,\n",
       "    \"queryPlanning\" : 868,\n",
       "    \"setOffsetRange\" : 1839,\n",
       "    \"triggerExecution\" : 33936,\n",
       "    \"walCommit\" : 75\n",
       "  },\n",
       "  \"eventTime\" : {\n",
       "    \"avg\" : \"2013-09-01T02:30:00.606Z\",\n",
       "    \"max\" : \"2013-09-01T03:18:40.000Z\",\n",
       "    \"min\" : \"2013-09-01T01:35:00.000Z\",\n",
       "    \"watermark\" : \"1970-01-01T00:00:00.000Z\"\n",
       "  },\n",
       "  \"stateOperators\" : [ {\n",
       "    \"numRowsTotal\" : 33,\n",
       "    \"numRowsUpdated\" : 33,\n",
       "    \"memo..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// readingsQuery.stop()\n",
    "readingsQuery.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Processing new data\",\n",
       "  \"isDataAvailable\" : true,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readingsQuery.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "statsQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@57537f72\n",
       "res4: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Getting offsets from KafkaV2[Subscribe[alert_1_stats]]\",\n",
       "  \"isDataAvailable\" : false,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val statsQuery = stats.writeStream.format(\"memory\").queryName(\"stats\").start()\n",
    "statsQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----------+---------+-----------+----------+-----+-------------------+\n",
      "|house_id|hour|       mean|       m2|   variance|   std_dev|count|            last_ts|\n",
      "+--------+----+-----------+---------+-----------+----------+-----+-------------------+\n",
      "|       6|   1|0.040391304|1.0778269|0.023951707|0.15476339|   46|2013-09-01 01:39:40|\n",
      "|       3|   1|0.042192988|1.8278245|0.032639723|0.18066467|   57|2013-09-01 01:51:20|\n",
      "|       7|   4|  2.4282303| 8838.108|  58.530518|  7.650524|  152|2013-09-01 04:04:00|\n",
      "|       2|   2|        0.0|      0.0|        0.0|       0.0|  180|2013-09-01 02:48:40|\n",
      "|       9|   4|  2.1643322|11563.692|  60.227566|  7.760642|  193|2013-09-01 04:36:40|\n",
      "|       8|   3|  11.005864|2108.1162|  10.593549| 3.2547731|  200|2013-09-01 03:44:20|\n",
      "|       2|   3|        0.0|      0.0|        0.0|       0.0|  172|2013-09-01 03:13:20|\n",
      "|       7|   1|        0.0|      0.0|        0.0|       0.0|   97|2013-09-01 01:47:00|\n",
      "|       1|   2|  2.9568827| 9.634522| 0.09539131|0.30885485|  102|2013-09-01 02:39:40|\n",
      "|       9|   1|        0.0|      0.0|        0.0|       0.0|  117|2013-09-01 01:36:40|\n",
      "|       1|   1|  2.9977632|2.6630826| 0.07197521|0.26828197|   38|2013-09-01 01:35:00|\n",
      "|       1|   3|    2.88233|13.820354| 0.13549367|0.36809465|  103|2013-09-01 03:10:20|\n",
      "|       7|   2|        0.0|      0.0|        0.0|       0.0|  215|2013-09-01 02:28:00|\n",
      "|       1|   4|  2.9505155|6.1806746|0.098105945|0.31321868|   64|2013-09-01 04:44:00|\n",
      "|       5|   4|  1.5440372|5636.7734|  29.982838| 5.4756584|  189|2013-09-01 04:37:40|\n",
      "|       7|   3|        0.0|      0.0|        0.0|       0.0|  220|2013-09-01 03:14:00|\n",
      "|       3|   3|        0.0|      0.0|        0.0|       0.0|  123|2013-09-01 03:19:40|\n",
      "|       9|   3|        0.0|      0.0|        0.0|       0.0|  267|2013-09-01 03:18:20|\n",
      "|       9|   2|        0.0|      0.0|        0.0|       0.0|  270|2013-09-01 02:14:20|\n",
      "|       8|   1| 11.3398285| 749.3814|   9.860282| 3.1401086|   77|2013-09-01 01:37:20|\n",
      "+--------+----+-----------+---------+-----------+----------+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Thread.sleep(10000)\n",
    "spark.sql(\"select * from stats\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: org.apache.spark.sql.streaming.StreamingQueryProgress =\n",
       "{\n",
       "  \"id\" : \"122ae385-ca91-44de-89a9-9825212998a1\",\n",
       "  \"runId\" : \"366dcf43-8124-4bb6-8038-459d7d185705\",\n",
       "  \"name\" : \"stats\",\n",
       "  \"timestamp\" : \"2020-03-02T12:51:12.133Z\",\n",
       "  \"batchId\" : 7,\n",
       "  \"numInputRows\" : 1,\n",
       "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
       "  \"processedRowsPerSecond\" : 0.05623031938821413,\n",
       "  \"durationMs\" : {\n",
       "    \"addBatch\" : 17107,\n",
       "    \"getBatch\" : 0,\n",
       "    \"getEndOffset\" : 0,\n",
       "    \"queryPlanning\" : 50,\n",
       "    \"setOffsetRange\" : 1,\n",
       "    \"triggerExecution\" : 17784,\n",
       "    \"walCommit\" : 99\n",
       "  },\n",
       "  \"stateOperators\" : [ ],\n",
       "  \"sources\" : [ {\n",
       "    \"description\" : \"KafkaV2[Subscribe[alert_1_stats]]\",\n",
       "    \"startOffset\" : {\n",
       "      \"alert_1_stats\" : {\n",
       "        \"0\" : 110\n",
       "      }\n",
       "    },\n",
       "    \"endOffset\" : {\n",
       "      \"alert_1_stats\" : {\n",
       "        \"..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// statsQuery.stop()\n",
    "statsQuery.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Processing new data\",\n",
       "  \"isDataAvailable\" : true,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statsQuery.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Both Data Streams\n",
    "\n",
    "\n",
    "We can only compare the readings with the past statistics. The `reading_ts` value of `readings_prepared` stream needs to be bigger than the `last_ts` value of the `alert_1_stats` stream. We need to ensure we are comparing the stats from the right `house_id` and `hour`.\n",
    "\n",
    "`owever, as the stream runs, the size of streaming state will keep growing indefinitely as all past input must be saved as any new input can match with any input from the past. To avoid unbounded state, you have to define additional join conditions such that indefinitely old inputs cannot match with future inputs and therefore can be cleared from the state. In other words, you will have to do the following additional steps in the join.`\n",
    "\n",
    "After joining, we need to ensure that we only compare the readings with their latest past statistics.\n",
    "\n",
    "The alert_1_stats stream is assumed to be slower to arrive compared to the readings_prepared stream, since it needs to wait at least 2 records to start calculating, and the default write trigger is every 30 seconds. \n",
    "\n",
    "\n",
    "The alert_1_stats may contain multiple rows with different `last_ts` and stats values as well, and we're going to solve that with an aggregation after joining. We're going to take the max `last_ts` that is still earlier than the `reading_ts` value.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "anomaly: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [message_id: string, reading_ts: timestamp ... 13 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Anomaly detection is done by getting the latest std_dev and mean value\n",
    "// Then act on it by comparing to the \n",
    "val anomaly = readings\n",
    "    .withColumnRenamed(\"house_id\", \"readings_house_id\")\n",
    "    .withWatermark(\"reading_ts\", joinWatermarkTime)\n",
    "    .join(\n",
    "        stats.withWatermark(\"last_ts\", joinWatermarkTime),\n",
    "        // Join conditions\n",
    "        $\"reading_ts\" > $\"last_ts\" &&\n",
    "        hour($\"reading_ts\") === $\"hour\" &&\n",
    "        $\"readings_house_id\" === $\"house_id\",\n",
    "        // Join type\n",
    "        \"inner\"\n",
    "    )\n",
    "    .filter($\"reading_value\" > $\"mean\" + $\"std_dev\")\n",
    "    .drop(\"readings_house_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peekQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@529e235\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val peekQuery = anomaly\n",
    "    .writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"anomaly\")\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------+------------+-------+------------+-----------------+--------+----+--------+---------+---------+--------+-----+-------------------+\n",
      "|message_id|         reading_ts|reading_value|reading_type|plug_id|household_id|readings_house_id|house_id|hour|    mean|       m2| variance| std_dev|count|            last_ts|\n",
      "+----------+-------------------+-------------+------------+-------+------------+-----------------+--------+----+--------+---------+---------+--------+-----+-------------------+\n",
      "|  53392037|2013-09-01 06:56:40|      138.496|           1|      1|           0|                1|       1|   6|26.55637|225374.89|2889.4216|53.75334|   79|2013-09-01 06:22:00|\n",
      "|  53025749|2013-09-01 06:53:00|      139.305|           1|      1|           0|                1|       1|   6|26.55637|225374.89|2889.4216|53.75334|   79|2013-09-01 06:22:00|\n",
      "|  56113430|2013-09-01 07:24:00|        0.788|           0|      0|           0|                4|       4|   7|     0.0|      0.0|      0.0|     0.0|   29|2013-09-01 07:05:40|\n",
      "|  58436890|2013-09-01 07:47:20|        0.788|           0|      0|           0|                4|       4|   7|     0.0|      0.0|      0.0|     0.0|   29|2013-09-01 07:05:40|\n",
      "|  58236932|2013-09-01 07:45:20|        0.356|           0|      1|           0|                4|       4|   7|     0.0|      0.0|      0.0|     0.0|   29|2013-09-01 07:05:40|\n",
      "|  56709240|2013-09-01 07:30:00|        0.356|           0|      1|           0|                4|       4|   7|     0.0|      0.0|      0.0|     0.0|   29|2013-09-01 07:05:40|\n",
      "|  55614449|2013-09-01 07:19:00|        0.356|           0|      1|           0|                4|       4|   7|     0.0|      0.0|      0.0|     0.0|   29|2013-09-01 07:05:40|\n",
      "|  59431072|2013-09-01 07:57:20|        0.788|           0|      0|           0|                4|       4|   7|     0.0|      0.0|      0.0|     0.0|   29|2013-09-01 07:05:40|\n",
      "|  58865489|2013-09-01 07:51:40|        0.788|           0|      0|           0|                4|       4|   7|     0.0|      0.0|      0.0|     0.0|   29|2013-09-01 07:05:40|\n",
      "|  58699892|2013-09-01 07:50:00|        0.982|           0|      2|           0|                4|       4|   7|     0.0|      0.0|      0.0|     0.0|   29|2013-09-01 07:05:40|\n",
      "|  56809815|2013-09-01 07:31:00|        0.356|           0|      1|           0|                4|       4|   7|     0.0|      0.0|      0.0|     0.0|   29|2013-09-01 07:05:40|\n",
      "|  56377599|2013-09-01 07:26:40|        0.982|           0|      2|           0|                4|       4|   7|     0.0|      0.0|      0.0|     0.0|   29|2013-09-01 07:05:40|\n",
      "|  57042784|2013-09-01 07:33:20|        0.788|           0|      0|           0|                4|       4|   7|     0.0|      0.0|      0.0|     0.0|   29|2013-09-01 07:05:40|\n",
      "|  59564179|2013-09-01 07:58:40|        0.356|           0|      1|           0|                4|       4|   7|     0.0|      0.0|      0.0|     0.0|   29|2013-09-01 07:05:40|\n",
      "|  54490254|2013-09-01 07:07:40|        0.982|           0|      2|           0|                4|       4|   7|     0.0|      0.0|      0.0|     0.0|   29|2013-09-01 07:05:40|\n",
      "|  59131081|2013-09-01 07:54:20|        0.356|           0|      1|           0|                4|       4|   7|     0.0|      0.0|      0.0|     0.0|   29|2013-09-01 07:05:40|\n",
      "|  59397981|2013-09-01 07:57:00|        0.788|           0|      0|           0|                4|       4|   7|     0.0|      0.0|      0.0|     0.0|   29|2013-09-01 07:05:40|\n",
      "|  54688885|2013-09-01 07:09:40|        0.788|           0|      0|           0|                4|       4|   7|     0.0|      0.0|      0.0|     0.0|   29|2013-09-01 07:05:40|\n",
      "|  57971447|2013-09-01 07:42:40|        0.788|           0|      0|           0|                4|       4|   7|     0.0|      0.0|      0.0|     0.0|   29|2013-09-01 07:05:40|\n",
      "|  58898353|2013-09-01 07:52:00|        0.788|           0|      0|           0|                4|       4|   7|     0.0|      0.0|      0.0|     0.0|   29|2013-09-01 07:05:40|\n",
      "+----------+-------------------+-------------+------------+-------+------------+-----------------+--------+----+--------+---------+---------+--------+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from anomaly\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: org.apache.spark.sql.streaming.StreamingQueryProgress =\n",
       "{\n",
       "  \"id\" : \"cd2b8764-9438-4402-b5a8-6ec321691974\",\n",
       "  \"runId\" : \"b2bcd94b-6fa9-466b-bd94-072e6d9ffa89\",\n",
       "  \"name\" : \"anomaly\",\n",
       "  \"timestamp\" : \"2020-03-02T12:51:19.851Z\",\n",
       "  \"batchId\" : 2,\n",
       "  \"numInputRows\" : 13150,\n",
       "  \"inputRowsPerSecond\" : 211.21444289179075,\n",
       "  \"processedRowsPerSecond\" : 233.37533497790477,\n",
       "  \"durationMs\" : {\n",
       "    \"addBatch\" : 55768,\n",
       "    \"getBatch\" : 0,\n",
       "    \"getEndOffset\" : 0,\n",
       "    \"queryPlanning\" : 381,\n",
       "    \"setOffsetRange\" : 3,\n",
       "    \"triggerExecution\" : 56347,\n",
       "    \"walCommit\" : 66\n",
       "  },\n",
       "  \"eventTime\" : {\n",
       "    \"avg\" : \"2013-09-01T12:33:51.803Z\",\n",
       "    \"max\" : \"2013-09-01T14:47:20.000Z\",\n",
       "    \"min\" : \"2013-09-01T10:20:40.000Z\",\n",
       "    \"watermark\" : \"2013-09-01T09:16:20.000Z\"\n",
       "  },\n",
       "  \"stateOperators\" : [ {\n",
       "    \"numRowsTotal..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peekQuery.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Processing new data\",\n",
       "  \"isDataAvailable\" : true,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peekQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "// peekQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to BigQuery\n",
    "\n",
    "Write to BigQuery once a minute. BigQuery inserts work better with mini-batched data and this ensures that we give enough time for `alert_1_stats` to accumulate. The [BigQuery connector](https://github.com/GoogleCloudDataproc/spark-bigquery-connector) is installed during cluster setup and loaded automatically when spark-shell session is initiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ingestQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@379de019\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ingestQuery = anomaly\n",
    "    .writeStream\n",
    "    .trigger(Trigger.ProcessingTime(outputTriggerTime))\n",
    "    .foreachBatch{ \n",
    "        (batchDF: DataFrame, batchId: Long) =>\n",
    "            batchDF.write.format(\"bigquery\")\n",
    "                .option(\"table\", bigQueryTargetTable)\n",
    "                .option(\"temporaryGcsBucket\", bigQueryTempBucket)\n",
    "                .mode(SaveMode.Append)\n",
    "                .save()\n",
    "    }.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestQuery.lastProgress.stateOperators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res42: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Getting offsets from KafkaV2[Subscribe[alert_1_stats]]\",\n",
       "  \"isDataAvailable\" : false,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingestQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}