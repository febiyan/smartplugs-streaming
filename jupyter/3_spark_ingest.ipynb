{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Ingestion\n",
    "\n",
    "This notebook showcases a simple data ingestion from Kafka's `readings_prepared` topic and \n",
    "\n",
    "## Setup\n",
    "\n",
    "Import all the required libraries and set the stream configuration variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://spark-ingest-m:8088/proxy/application_1583161289898_0001\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = yarn, app id = application_1583161289898_0001)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.streaming.Trigger\n",
       "import org.apache.spark.sql._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kafkaBootstrapServer: String = kafka-m:9092\n",
       "kafkaReadingsTopic: String = readings_prepared\n",
       "kafkaDedupWatermarkTime: String = 1 minute\n",
       "bigQueryTargetTable: String = smartplugs.readings\n",
       "bigQueryTempBucket: String = pandora-sde-case/ingest\n",
       "outputTriggerTime: String = 1 minute\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kafkaBootstrapServer = \"kafka-m:9092\"\n",
    "val kafkaReadingsTopic = \"readings_prepared\"\n",
    "val kafkaDedupWatermarkTime = \"1 minute\"\n",
    "val bigQueryTargetTable = \"smartplugs.readings\"\n",
    "val bigQueryTempBucket = \"pandora-sde-case/ingest\"\n",
    "val outputTriggerTime = \"1 minute\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define The Required Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readingsSchema: org.apache.spark.sql.types.StructType = StructType(StructField(message_id,StringType,false), StructField(reading_ts,TimestampType,false), StructField(reading_value,FloatType,false), StructField(reading_type,IntegerType,false), StructField(plug_id,IntegerType,false), StructField(household_id,IntegerType,false), StructField(house_id,IntegerType,false))\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// This will be used to give the source `readings_prepared` stream data a schema\n",
    "val readingsSchema = StructType(Seq(\n",
    "    StructField(\"message_id\", StringType, false),\n",
    "    StructField(\"reading_ts\", TimestampType, false),\n",
    "    StructField(\"reading_value\", FloatType, false),\n",
    "    StructField(\"reading_type\", IntegerType, false),\n",
    "    StructField(\"plug_id\", IntegerType, false),\n",
    "    StructField(\"household_id\", IntegerType, false),\n",
    "    StructField(\"house_id\", IntegerType, false)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and Parse The Input Data Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readings: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [message_id: string, reading_ts: timestamp ... 5 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Drop duplicates if seen in arbitrary 1 seconds watermark. Bounds are necessary so that Spark does not store ALL records in the stateval readings = spark\n",
    "val readings = spark\n",
    "    .readStream \n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n",
    "    .option(\"subscribe\", kafkaReadingsTopic)\n",
    "    .load()\n",
    "    .selectExpr(\"CAST(value AS STRING)\")\n",
    "    .select(from_json($\"value\", readingsSchema).as(\"data\"))\n",
    "    .select($\"data.*\")\n",
    "    .withWatermark(\"reading_ts\", kafkaDedupWatermarkTime) \n",
    "    .dropDuplicates()\n",
    "    .filter($\"reading_type\" === 1) // Only take the \"current load\" measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peek at The Input Data Streams\n",
    "\n",
    "##### Readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readingsQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@101d3c83\n",
       "res0: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Processing new data\",\n",
       "  \"isDataAvailable\" : true,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val readingsQuery = readings.writeStream.format(\"memory\").queryName(\"readings\").start()\n",
    "Thread.sleep(10000)\n",
    "readingsQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------+------------+-------+------------+--------+\n",
      "|message_id|         reading_ts|reading_value|reading_type|plug_id|household_id|house_id|\n",
      "+----------+-------------------+-------------+------------+-------+------------+--------+\n",
      "| 219686108|2013-09-02 10:48:20|          0.0|           1|      1|           0|       3|\n",
      "| 220252836|2013-09-02 10:54:00|          0.0|           1|      0|           0|       4|\n",
      "| 219853298|2013-09-02 10:50:00|          0.0|           1|      1|           0|       9|\n",
      "| 221116806|2013-09-02 11:02:40|      136.521|           1|      1|           0|       1|\n",
      "| 222516718|2013-09-02 11:16:40|       67.329|           1|      0|           0|       8|\n",
      "| 222782967|2013-09-02 11:19:20|       60.253|           1|      0|           0|       8|\n",
      "| 222849589|2013-09-02 11:20:00|          0.0|           1|      1|           0|       9|\n",
      "| 224446503|2013-09-02 11:36:00|          0.0|           1|      2|           0|       7|\n",
      "| 225245702|2013-09-02 11:44:00|          0.0|           1|      0|           0|       5|\n",
      "| 227775956|2013-09-02 12:09:20|          0.0|           1|      1|           0|       5|\n",
      "| 227308542|2013-09-02 12:04:40|          0.0|           1|      2|           0|       2|\n",
      "| 227141058|2013-09-02 12:03:00|       10.645|           1|      2|           0|       0|\n",
      "| 227375853|2013-09-02 12:05:20|          0.0|           1|      1|           0|       5|\n",
      "| 226777239|2013-09-02 11:59:20|          0.0|           1|      2|           0|       7|\n",
      "| 229172357|2013-09-02 12:23:20|          0.0|           1|      2|           0|       2|\n",
      "| 221018384|2013-09-02 11:01:40|          0.0|           1|      0|           0|       9|\n",
      "| 221917880|2013-09-02 11:10:40|          0.0|           1|      2|           0|       5|\n",
      "| 222217504|2013-09-02 11:13:40|          0.0|           1|      0|           0|       7|\n",
      "| 223648548|2013-09-02 11:28:00|          0.0|           1|      1|           0|       5|\n",
      "| 224412758|2013-09-02 11:35:40|          0.0|           1|      1|           0|       3|\n",
      "+----------+-------------------+-------------+------------+-------+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from readings\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: org.apache.spark.sql.streaming.StreamingQueryProgress =\n",
       "{\n",
       "  \"id\" : \"f9ecee83-8f34-4875-b682-c10c7344564b\",\n",
       "  \"runId\" : \"54100149-2487-44fa-8aff-76654e1807dd\",\n",
       "  \"name\" : \"readings\",\n",
       "  \"timestamp\" : \"2020-03-02T15:38:27.730Z\",\n",
       "  \"batchId\" : 4,\n",
       "  \"numInputRows\" : 3126,\n",
       "  \"inputRowsPerSecond\" : 212.01844818231146,\n",
       "  \"processedRowsPerSecond\" : 19.3442988155794,\n",
       "  \"durationMs\" : {\n",
       "    \"addBatch\" : 161285,\n",
       "    \"getBatch\" : 0,\n",
       "    \"getEndOffset\" : 0,\n",
       "    \"queryPlanning\" : 169,\n",
       "    \"setOffsetRange\" : 4,\n",
       "    \"triggerExecution\" : 161598,\n",
       "    \"walCommit\" : 90\n",
       "  },\n",
       "  \"eventTime\" : {\n",
       "    \"avg\" : \"2013-09-02T17:03:07.236Z\",\n",
       "    \"max\" : \"2013-09-02T17:43:00.000Z\",\n",
       "    \"min\" : \"2013-09-02T16:29:40.000Z\",\n",
       "    \"watermark\" : \"2013-09-02T16:48:20.000Z\"\n",
       "  },\n",
       "  \"stateOperators\" : [ {\n",
       "    \"numRowsTotal..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// readingsQuery.stop()\n",
    "readingsQuery.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Processing new data\",\n",
       "  \"isDataAvailable\" : true,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readingsQuery.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to BigQuery\n",
    "\n",
    "Write to BigQuery once a minute. BigQuery works better with batched data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ingestQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@7c3fcc66\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ingestQuery = readings\n",
    "    .writeStream\n",
    "    .trigger(Trigger.ProcessingTime(outputTriggerTime))\n",
    "    .foreachBatch{ \n",
    "        (batchDF: DataFrame, batchId: Long) =>\n",
    "            batchDF.write.format(\"bigquery\")\n",
    "                .option(\"table\", bigQueryTargetTable)\n",
    "                .option(\"temporaryGcsBucket\", bigQueryTempBucket)\n",
    "                .mode(SaveMode.Append)\n",
    "                .save()\n",
    "    }.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res20: org.apache.spark.sql.streaming.StreamingQueryProgress =\n",
       "{\n",
       "  \"id\" : \"14686b54-a3bf-46b6-9561-1af1d282eb8e\",\n",
       "  \"runId\" : \"6fb5db6e-b637-4f38-a55b-b80e0db28ad1\",\n",
       "  \"name\" : null,\n",
       "  \"timestamp\" : \"2020-03-02T15:38:13.069Z\",\n",
       "  \"batchId\" : 1,\n",
       "  \"numInputRows\" : 11229,\n",
       "  \"inputRowsPerSecond\" : 194.37087812224127,\n",
       "  \"processedRowsPerSecond\" : 61.10854126418329,\n",
       "  \"durationMs\" : {\n",
       "    \"addBatch\" : 183051,\n",
       "    \"getBatch\" : 0,\n",
       "    \"getEndOffset\" : 0,\n",
       "    \"queryPlanning\" : 177,\n",
       "    \"setOffsetRange\" : 4,\n",
       "    \"triggerExecution\" : 183755,\n",
       "    \"walCommit\" : 475\n",
       "  },\n",
       "  \"eventTime\" : {\n",
       "    \"avg\" : \"2013-09-02T14:37:02.805Z\",\n",
       "    \"max\" : \"2013-09-02T16:49:20.000Z\",\n",
       "    \"min\" : \"2013-09-02T12:34:20.000Z\",\n",
       "    \"watermark\" : \"1970-01-01T00:00:00.000Z\"\n",
       "  },\n",
       "  \"stateOperators\" : [ {\n",
       "    \"numRowsTotal\" :..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingestQuery.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res21: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Processing new data\",\n",
       "  \"isDataAvailable\" : true,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingestQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// ingestQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}