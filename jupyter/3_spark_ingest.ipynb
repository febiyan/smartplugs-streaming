{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Ingestion\n",
    "\n",
    "This notebook showcases a simple data ingestion from Kafka's `readings_prepared` topic and \n",
    "\n",
    "## Setup\n",
    "\n",
    "Import all the required libraries and set the stream configuration variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://spark-alert-1-detect-m:8088/proxy/application_1583148924695_0001\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = yarn, app id = application_1583148924695_0001)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.streaming.Trigger\n",
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kafkaBootstrapServer: String = kafka-m:9092\n",
       "kafkaReadingsTopic: String = readings_prepared\n",
       "kafkaStatsTopic: String = alert_1_stats\n",
       "kafkaDedupWatermarkTime: String = 1 minute\n",
       "joinWatermarkTime: String = 1 minute\n",
       "bigQueryTargetTable: String = smartplugs.alert_1_anomaly\n",
       "bigQueryTempBucket: String = pandora-sde-case\n",
       "outputTriggerTime: String = 1 minute\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kafkaBootstrapServer = \"kafka-m:9092\"\n",
    "val kafkaReadingsTopic = \"readings_prepared\"\n",
    "val kafkaDedupWatermarkTime = \"1 minute\"\n",
    "val bigQueryTargetTable = \"smartplugs.readings\"\n",
    "val bigQueryTempBucket = \"pandora-sde-case/ingest\"\n",
    "val outputTriggerTime = \"1 minute\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define The Required Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readingsSchema: org.apache.spark.sql.types.StructType = StructType(StructField(message_id,StringType,false), StructField(reading_ts,TimestampType,false), StructField(reading_value,FloatType,false), StructField(reading_type,IntegerType,false), StructField(plug_id,IntegerType,false), StructField(household_id,IntegerType,false), StructField(house_id,IntegerType,false))\n",
       "statsSchema: org.apache.spark.sql.types.StructType = StructType(StructField(house_id,IntegerType,false), StructField(hour,IntegerType,false), StructField(mean,FloatType,false), StructField(m2,FloatType,false), StructField(variance,FloatType,false), StructField(std_dev,FloatType,false), StructField(count,LongType,false), StructField(last_ts,TimestampType,false))\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// This will be used to give the source `readings_prepared` stream data a schema\n",
    "val readingsSchema = StructType(Seq(\n",
    "    StructField(\"message_id\", StringType, false),\n",
    "    StructField(\"reading_ts\", TimestampType, false),\n",
    "    StructField(\"reading_value\", FloatType, false),\n",
    "    StructField(\"reading_type\", IntegerType, false),\n",
    "    StructField(\"plug_id\", IntegerType, false),\n",
    "    StructField(\"household_id\", IntegerType, false),\n",
    "    StructField(\"house_id\", IntegerType, false)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and Parse The Input Data Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://spark-alert-1-stats-m:8088/proxy/application_1583148921700_0002\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = yarn, app id = application_1583148921700_0002)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "<console>",
     "evalue": "28: error: not found: value kafkaBootstrapServer",
     "output_type": "error",
     "traceback": [
      "<console>:28: error: not found: value kafkaBootstrapServer",
      "           .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)",
      "                                              ^",
      "<console>:29: error: not found: value kafkaReadingsTopic",
      "           .option(\"subscribe\", kafkaReadingsTopic)",
      "                                ^",
      "<console>:32: error: not found: value readingsSchema",
      "           .select(from_json($\"value\", readingsSchema).as(\"data\"))",
      "                                       ^",
      "<console>:34: error: not found: value kafkaDedupWatermarkTime",
      "           .withWatermark(\"reading_ts\", kafkaDedupWatermarkTime)",
      "                                        ^",
      ""
     ]
    }
   ],
   "source": [
    "// Drop duplicates if seen in arbitrary 1 seconds watermark. Bounds are necessary so that Spark does not store ALL records in the stateval readings = spark\n",
    "val readings = spark\n",
    "    .readStream \n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n",
    "    .option(\"subscribe\", kafkaReadingsTopic)\n",
    "    .load()\n",
    "    .selectExpr(\"CAST(value AS STRING)\")\n",
    "    .select(from_json($\"value\", readingsSchema).as(\"data\"))\n",
    "    .select($\"\")\n",
    "    .withWatermark(\"reading_ts\", kafkaDedupWatermarkTime) \n",
    "    .dropDuplicates()\n",
    "    .filter($\"reading_type\" === 1) // Only take the \"current load\" measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peek at The Input Data Streams\n",
    "\n",
    "##### Readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readingsQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@7e4fdee9\n",
       "res0: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Getting offsets from KafkaV2[Subscribe[readings_prepared]]\",\n",
       "  \"isDataAvailable\" : false,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val readingsQuery = readings.writeStream.format(\"memory\").queryName(\"readings\").start()\n",
    "Thread.sleep(10000)\n",
    "readingsQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------+------------+-------+------------+--------+\n",
      "|message_id|         reading_ts|reading_value|reading_type|plug_id|household_id|house_id|\n",
      "+----------+-------------------+-------------+------------+-------+------------+--------+\n",
      "|  21267584|2013-09-01 01:35:00|       35.099|           0|      1|           0|       1|\n",
      "|  21371713|2013-09-01 01:36:00|       11.757|           0|      2|           0|       0|\n",
      "|  22405161|2013-09-01 01:46:20|        0.355|           0|      1|           0|       4|\n",
      "|  22471510|2013-09-01 01:47:00|          0.0|           1|      0|           0|       4|\n",
      "|  22636827|2013-09-01 01:48:40|          0.0|           1|      1|           0|       3|\n",
      "|  23903171|2013-09-01 02:01:20|        0.161|           0|      0|           0|       9|\n",
      "|  24134854|2013-09-01 02:03:40|        9.853|           1|      2|           0|       0|\n",
      "|  24302832|2013-09-01 02:05:20|         2.25|           0|      1|           0|       7|\n",
      "|  24435084|2013-09-01 02:06:40|          0.0|           1|      0|           0|       2|\n",
      "|  25134972|2013-09-01 02:13:40|          0.0|           1|      0|           0|       5|\n",
      "|  25268752|2013-09-01 02:15:00|          0.0|           1|      2|           0|       5|\n",
      "|  25599296|2013-09-01 02:18:20|        3.355|           1|      1|           0|       1|\n",
      "|  25601331|2013-09-01 02:18:20|         2.25|           0|      1|           0|       5|\n",
      "|  25832881|2013-09-01 02:20:40|        0.161|           0|      2|           0|       2|\n",
      "|  26399731|2013-09-01 02:26:20|        0.982|           0|      2|           0|       4|\n",
      "|  26566167|2013-09-01 02:28:00|          0.0|           1|      1|           0|       3|\n",
      "|  26699760|2013-09-01 02:29:20|          0.0|           1|      2|           0|       9|\n",
      "|  26932542|2013-09-01 02:31:40|        0.982|           0|      0|           0|       7|\n",
      "|  27431839|2013-09-01 02:36:40|         2.25|           0|      1|           0|       7|\n",
      "|  27631410|2013-09-01 02:38:40|          0.0|           1|      2|           0|       9|\n",
      "+----------+-------------------+-------------+------------+-------+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from readings\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: org.apache.spark.sql.streaming.StreamingQueryProgress =\n",
       "{\n",
       "  \"id\" : \"9a99c6a0-e13f-4654-824c-d8fd89919a13\",\n",
       "  \"runId\" : \"9b858307-236e-4829-8378-b927c61ba527\",\n",
       "  \"name\" : \"readings\",\n",
       "  \"timestamp\" : \"2020-03-02T12:48:06.186Z\",\n",
       "  \"batchId\" : 0,\n",
       "  \"numInputRows\" : 33,\n",
       "  \"processedRowsPerSecond\" : 0.9723613648417703,\n",
       "  \"durationMs\" : {\n",
       "    \"addBatch\" : 30895,\n",
       "    \"getBatch\" : 11,\n",
       "    \"getEndOffset\" : 0,\n",
       "    \"queryPlanning\" : 868,\n",
       "    \"setOffsetRange\" : 1839,\n",
       "    \"triggerExecution\" : 33936,\n",
       "    \"walCommit\" : 75\n",
       "  },\n",
       "  \"eventTime\" : {\n",
       "    \"avg\" : \"2013-09-01T02:30:00.606Z\",\n",
       "    \"max\" : \"2013-09-01T03:18:40.000Z\",\n",
       "    \"min\" : \"2013-09-01T01:35:00.000Z\",\n",
       "    \"watermark\" : \"1970-01-01T00:00:00.000Z\"\n",
       "  },\n",
       "  \"stateOperators\" : [ {\n",
       "    \"numRowsTotal\" : 33,\n",
       "    \"numRowsUpdated\" : 33,\n",
       "    \"memo..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// readingsQuery.stop()\n",
    "readingsQuery.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Processing new data\",\n",
       "  \"isDataAvailable\" : true,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readingsQuery.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to BigQuery\n",
    "\n",
    "Write to BigQuery once a minute. BigQuery works better with batched data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ingestQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@379de019\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ingestQuery = readings\n",
    "    .writeStream\n",
    "    .trigger(Trigger.ProcessingTime(outputTriggerTime))\n",
    "    .foreachBatch{ \n",
    "        (batchDF: DataFrame, batchId: Long) =>\n",
    "            batchDF.write.format(\"bigquery\")\n",
    "                .option(\"table\", bigQueryTargetTable)\n",
    "                .option(\"temporaryGcsBucket\", bigQueryTempBucket)\n",
    "                .mode(SaveMode.Append)\n",
    "                .save()\n",
    "    }.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestQuery.lastProgress.stateOperators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res42: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Getting offsets from KafkaV2[Subscribe[alert_1_stats]]\",\n",
       "  \"isDataAvailable\" : false,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingestQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "// ingestQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}