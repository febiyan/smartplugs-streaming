{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Ingestion\n",
    "\n",
    "This notebook showcases a simple data ingestion from Kafka's `readings_prepared` topic and \n",
    "\n",
    "## Setup\n",
    "\n",
    "Import all the required libraries and set the stream configuration variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://spark-ingest-m:8088/proxy/application_1583313192730_0001\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = yarn, app id = application_1583313192730_0001)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.streaming.Trigger\n",
       "import org.apache.spark.sql._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kafkaBootstrapServer: String = kafka-m:9092\n",
       "kafkaReadingsTopic: String = readings_prepared\n",
       "kafkaDedupWatermarkTime: String = 1 minute\n",
       "bigQueryTargetTable: String = smartplugs.readings\n",
       "bigQueryTempBucket: String = pandora-sde-case/ingest\n",
       "outputTriggerTime: String = 1 minute\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kafkaBootstrapServer = \"kafka-m:9092\"\n",
    "val kafkaReadingsTopic = \"readings_prepared\"\n",
    "val kafkaDedupWatermarkTime = \"1 minute\"\n",
    "val bigQueryTargetTable = \"smartplugs.readings\"\n",
    "val bigQueryTempBucket = \"pandora-sde-case/ingest\"\n",
    "val outputTriggerTime = \"1 minute\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define The Required Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readingsSchema: org.apache.spark.sql.types.StructType = StructType(StructField(message_id,StringType,false), StructField(reading_ts,TimestampType,false), StructField(reading_value,FloatType,false), StructField(reading_type,IntegerType,false), StructField(plug_id,IntegerType,false), StructField(household_id,IntegerType,false), StructField(house_id,IntegerType,false))\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// This will be used to give the source `readings_prepared` stream data a schema\n",
    "val readingsSchema = StructType(Seq(\n",
    "    StructField(\"message_id\", StringType, false),\n",
    "    StructField(\"reading_ts\", TimestampType, false),\n",
    "    StructField(\"reading_value\", FloatType, false),\n",
    "    StructField(\"reading_type\", IntegerType, false),\n",
    "    StructField(\"plug_id\", IntegerType, false),\n",
    "    StructField(\"household_id\", IntegerType, false),\n",
    "    StructField(\"house_id\", IntegerType, false)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and Parse The Input Data Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readings: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [message_id: string, reading_ts: timestamp ... 5 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Drop duplicates if seen in arbitrary 1 seconds watermark. Bounds are necessary so that Spark does not store ALL records in the stateval readings = spark\n",
    "val readings = spark\n",
    "    .readStream \n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n",
    "    .option(\"subscribe\", kafkaReadingsTopic)\n",
    "    .load()\n",
    "    .selectExpr(\"CAST(value AS STRING)\")\n",
    "    .select(from_json($\"value\", readingsSchema).as(\"data\"))\n",
    "    .select($\"data.*\")\n",
    "    .withWatermark(\"reading_ts\", kafkaDedupWatermarkTime) \n",
    "    .dropDuplicates()\n",
    "    .filter($\"reading_type\" === 1) // Only take the \"current load\" measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peek at The Input Data Streams\n",
    "\n",
    "##### Readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readingsQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@607d6f30\n",
       "res0: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Processing new data\",\n",
       "  \"isDataAvailable\" : true,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val readingsQuery = readings.writeStream.format(\"memory\").queryName(\"readings\").start()\n",
    "Thread.sleep(10000)\n",
    "readingsQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------+------------+-------+------------+--------+\n",
      "|message_id|         reading_ts|reading_value|reading_type|plug_id|household_id|house_id|\n",
      "+----------+-------------------+-------------+------------+-------+------------+--------+\n",
      "|  47669710|2013-09-01 05:59:20|          0.0|           1|      1|           0|       5|\n",
      "|  48068742|2013-09-01 06:03:20|          0.0|           1|      1|           0|       5|\n",
      "|  49066547|2013-09-01 06:13:20|          0.0|           1|      1|           0|       5|\n",
      "|  49233526|2013-09-01 06:15:00|          0.0|           1|      0|           0|       9|\n",
      "|  51762125|2013-09-01 06:40:20|          0.0|           1|      1|           0|       7|\n",
      "|  50863413|2013-09-01 06:31:20|          0.0|           1|      1|           0|       7|\n",
      "|  50961779|2013-09-01 06:32:20|       42.532|           1|      2|           0|       0|\n",
      "|  52861089|2013-09-01 06:51:20|          0.0|           1|      2|           0|       9|\n",
      "|  53557364|2013-09-01 06:58:20|          0.0|           1|      2|           0|       2|\n",
      "|  53457640|2013-09-01 06:57:20|      164.371|           1|      1|           0|       1|\n",
      "|  54158098|2013-09-01 07:04:20|          0.0|           1|      2|           0|       9|\n",
      "|  50596811|2013-09-01 06:28:40|        6.162|           1|      1|           0|       8|\n",
      "|  55748092|2013-09-01 07:20:20|       53.164|           1|      0|           0|       8|\n",
      "|  57638304|2013-09-01 07:39:20|          0.0|           1|      2|           0|       2|\n",
      "|  48268156|2013-09-01 06:05:20|          0.0|           1|      1|           0|       3|\n",
      "|  48336056|2013-09-01 06:06:00|          0.0|           1|      2|           0|       9|\n",
      "|  47003516|2013-09-01 05:52:40|          0.0|           1|      1|           0|       4|\n",
      "|  50463285|2013-09-01 06:27:20|          0.0|           1|      1|           0|       3|\n",
      "|  52527172|2013-09-01 06:48:00|          0.0|           1|      0|           0|       3|\n",
      "|  52360334|2013-09-01 06:46:20|          0.0|           1|      2|           0|       2|\n",
      "+----------+-------------------+-------------+------------+-------+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from readings\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: org.apache.spark.sql.streaming.StreamingQueryProgress =\n",
       "{\n",
       "  \"id\" : \"b97e0a73-27c5-4471-85fb-b03a52580ed7\",\n",
       "  \"runId\" : \"8dae5afd-f3d0-4860-9896-b8ce54e1518c\",\n",
       "  \"name\" : \"readings\",\n",
       "  \"timestamp\" : \"2020-03-04T09:19:39.012Z\",\n",
       "  \"batchId\" : 0,\n",
       "  \"numInputRows\" : 0,\n",
       "  \"processedRowsPerSecond\" : 0.0,\n",
       "  \"durationMs\" : {\n",
       "    \"addBatch\" : 28460,\n",
       "    \"getBatch\" : 10,\n",
       "    \"getEndOffset\" : 0,\n",
       "    \"queryPlanning\" : 887,\n",
       "    \"setOffsetRange\" : 1637,\n",
       "    \"triggerExecution\" : 31260,\n",
       "    \"walCommit\" : 64\n",
       "  },\n",
       "  \"eventTime\" : {\n",
       "    \"watermark\" : \"1970-01-01T00:00:00.000Z\"\n",
       "  },\n",
       "  \"stateOperators\" : [ {\n",
       "    \"numRowsTotal\" : 0,\n",
       "    \"numRowsUpdated\" : 0,\n",
       "    \"memoryUsedBytes\" : 44599,\n",
       "    \"customMetrics\" : {\n",
       "      \"loadedMapCacheHitCount\" : 0,\n",
       "      \"loadedMapCacheMissCount\" : 0,\n",
       "      \"stateOnCurre..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// readingsQuery.stop()\n",
    "readingsQuery.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Processing new data\",\n",
       "  \"isDataAvailable\" : true,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readingsQuery.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to BigQuery\n",
    "\n",
    "Write to BigQuery once a minute. BigQuery works better with batched data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ingestQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@99f47e8\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ingestQuery = readings\n",
    "    .writeStream\n",
    "    .trigger(Trigger.ProcessingTime(outputTriggerTime))\n",
    "    .foreachBatch{ \n",
    "        (batchDF: DataFrame, batchId: Long) =>\n",
    "            batchDF.write.format(\"bigquery\")\n",
    "                .option(\"table\", bigQueryTargetTable)\n",
    "                .option(\"temporaryGcsBucket\", bigQueryTempBucket)\n",
    "                .mode(SaveMode.Append)\n",
    "                .save()\n",
    "    }.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: org.apache.spark.sql.streaming.StreamingQueryProgress =\n",
       "{\n",
       "  \"id\" : \"bbc4556e-1488-4412-aae4-babad5501d44\",\n",
       "  \"runId\" : \"2e442a93-ba5d-4cbc-a7ac-5f4991694a42\",\n",
       "  \"name\" : null,\n",
       "  \"timestamp\" : \"2020-03-04T09:20:13.066Z\",\n",
       "  \"batchId\" : 0,\n",
       "  \"numInputRows\" : 28,\n",
       "  \"processedRowsPerSecond\" : 0.28718832374329467,\n",
       "  \"durationMs\" : {\n",
       "    \"addBatch\" : 96921,\n",
       "    \"getBatch\" : 0,\n",
       "    \"getEndOffset\" : 1,\n",
       "    \"queryPlanning\" : 221,\n",
       "    \"setOffsetRange\" : 203,\n",
       "    \"triggerExecution\" : 97497,\n",
       "    \"walCommit\" : 106\n",
       "  },\n",
       "  \"eventTime\" : {\n",
       "    \"avg\" : \"2013-09-01T07:24:01.428Z\",\n",
       "    \"max\" : \"2013-09-01T07:41:00.000Z\",\n",
       "    \"min\" : \"2013-09-01T07:08:00.000Z\",\n",
       "    \"watermark\" : \"1970-01-01T00:00:00.000Z\"\n",
       "  },\n",
       "  \"stateOperators\" : [ {\n",
       "    \"numRowsTotal\" : 28,\n",
       "    \"numRowsUpdated\" : 28,\n",
       "    \"memoryUsed..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingestQuery.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Processing new data\",\n",
       "  \"isDataAvailable\" : true,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingestQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "// ingestQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}