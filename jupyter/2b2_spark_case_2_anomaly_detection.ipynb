{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2 Part 2: Anomaly Detection\n",
    "\n",
    "This notebook showcases the 2nd part of the analytics use case to be tackled in a real-time alerting system:\n",
    "\n",
    "`Hourly consumption for a household is higher than 1 standard deviation of mean consumption across all households within that particular hour on that day.`\n",
    "\n",
    "The second part here is to read the data stream from both `readings_prepared` and `alert_2_stats` and use them to detect data anomalies -- the ones that go above 1 standard deviation from the mean. The detected anomalies are then stored in a persistent data store, which in this case is Google BigQuery.\n",
    "\n",
    "BigQuery is chosen for the fit of further analytical queries down the line. We might be interested to do some BI or advanced analysis down the line. It is also serverless -- we just need to define the Datasets and Tables.\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "Import all the required libraries and set the stream configuration variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://spark-alert-2-detect-m:8088/proxy/application_1583313187361_0001\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = yarn, app id = application_1583313187361_0001)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.streaming.Trigger\n",
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kafkaBootstrapServer: String = kafka-m:9092\n",
       "kafkaReadingsTopic: String = readings_prepared\n",
       "kafkaStatsTopic: String = alert_2_stats\n",
       "kafkaDedupWatermarkTime: String = 1 minute\n",
       "joinWatermarkTime: String = 1 minute\n",
       "bigQueryTargetTable: String = smartplugs.alert_2_anomaly\n",
       "bigQueryTempBucket: String = pandora-sde-case/alert_2\n",
       "outputTriggerTime: String = 1 minute\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kafkaBootstrapServer = \"kafka-m:9092\"\n",
    "val kafkaReadingsTopic = \"readings_prepared\"\n",
    "val kafkaStatsTopic = \"alert_2_stats\"\n",
    "val kafkaDedupWatermarkTime = \"1 minute\"\n",
    "val joinWatermarkTime = \"1 minute\"\n",
    "val bigQueryTargetTable = \"smartplugs.alert_2_anomaly\"\n",
    "val bigQueryTempBucket = \"pandora-sde-case/alert_2\"\n",
    "val outputTriggerTime = \"1 minute\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define The Required Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readingsSchema: org.apache.spark.sql.types.StructType = StructType(StructField(message_id,StringType,false), StructField(reading_ts,TimestampType,false), StructField(reading_value,FloatType,false), StructField(reading_type,IntegerType,false), StructField(plug_id,IntegerType,false), StructField(household_id,IntegerType,false), StructField(house_id,IntegerType,false))\n",
       "statsSchema: org.apache.spark.sql.types.StructType = StructType(StructField(day,StringType,false), StructField(hour,IntegerType,false), StructField(mean,FloatType,false), StructField(m2,FloatType,false), StructField(variance,FloatType,false), StructField(std_dev,FloatType,false), StructField(count,LongType,false), StructField(last_ts,TimestampType,false))\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// This will be used to give the source `readings_prepared` stream data a schema\n",
    "val readingsSchema = StructType(Seq(\n",
    "    StructField(\"message_id\", StringType, false),\n",
    "    StructField(\"reading_ts\", TimestampType, false),\n",
    "    StructField(\"reading_value\", FloatType, false),\n",
    "    StructField(\"reading_type\", IntegerType, false),\n",
    "    StructField(\"plug_id\", IntegerType, false),\n",
    "    StructField(\"household_id\", IntegerType, false),\n",
    "    StructField(\"house_id\", IntegerType, false)\n",
    "))\n",
    "\n",
    "val statsSchema = StructType(Seq(\n",
    "    StructField(\"day\", StringType, false),\n",
    "    StructField(\"hour\", IntegerType, false),\n",
    "    StructField(\"mean\", FloatType, false),\n",
    "    StructField(\"m2\", FloatType, false),\n",
    "    StructField(\"variance\", FloatType, false),\n",
    "    StructField(\"std_dev\", FloatType, false),\n",
    "    StructField(\"count\", LongType, false),\n",
    "    StructField(\"last_ts\", TimestampType, false)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and Parse The Input Data Streams\n",
    "\n",
    "There are 2 input streams this time, `readings_prep` and `alert_1_stats` topic. They will be joined to detect anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readings: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [message_id: string, reading_ts: timestamp ... 5 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Drop duplicates if seen in an arbitrary watermark. Bounds are necessary so that Spark does not store \n",
    "// ALL records in the state memory\n",
    "val readings = spark\n",
    "    .readStream \n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n",
    "    .option(\"subscribe\", kafkaReadingsTopic)\n",
    "    .load()\n",
    "    .selectExpr(\"CAST(value AS STRING)\")\n",
    "    .select(from_json($\"value\", readingsSchema).as(\"data\"))\n",
    "    .select($\"data.*\")\n",
    "    .withWatermark(\"reading_ts\", kafkaDedupWatermarkTime) \n",
    "    .dropDuplicates()\n",
    "    .filter($\"reading_type\" === 1) // Only take the \"current load\" measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stats: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [day: string, hour: int ... 3 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// We don't need to deduplicate the stats, but we need to drop records with mean=/0\n",
    "val stats = spark\n",
    "    .readStream \n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", kafkaBootstrapServer)\n",
    "    .option(\"subscribe\", kafkaStatsTopic)\n",
    "    .load()\n",
    "    .selectExpr(\"CAST(value AS STRING)\")\n",
    "    .select(from_json($\"value\", statsSchema).as(\"data\"))\n",
    "    .select($\"data.day\", $\"data.hour\", $\"data.mean\", $\"data.std_dev\", $\"data.last_ts\")\n",
    "    .filter($\"mean\" > 0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peek at The Input Data Streams\n",
    "\n",
    "##### Readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readingsQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@3b36f0b0\n",
       "res0: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Processing new data\",\n",
       "  \"isDataAvailable\" : true,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val readingsQuery = readings.writeStream.format(\"memory\").queryName(\"readings\").start()\n",
    "Thread.sleep(10000)\n",
    "readingsQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------+------------+-------+------------+--------+\n",
      "|message_id|         reading_ts|reading_value|reading_type|plug_id|household_id|house_id|\n",
      "+----------+-------------------+-------------+------------+-------+------------+--------+\n",
      "|  47669710|2013-09-01 05:59:20|          0.0|           1|      1|           0|       5|\n",
      "|  48068742|2013-09-01 06:03:20|          0.0|           1|      1|           0|       5|\n",
      "|  48268156|2013-09-01 06:05:20|          0.0|           1|      1|           0|       3|\n",
      "|  48767019|2013-09-01 06:10:20|          0.0|           1|      0|           0|       6|\n",
      "|  47270293|2013-09-01 05:55:20|          0.0|           1|      1|           0|       7|\n",
      "|  48401708|2013-09-01 06:06:40|        8.255|           1|      1|           0|       8|\n",
      "|  47836359|2013-09-01 06:01:00|          0.0|           1|      2|           0|       7|\n",
      "|  47270400|2013-09-01 05:55:20|          0.0|           1|      2|           0|       9|\n",
      "|  47303497|2013-09-01 05:55:40|        8.041|           1|      1|           0|       8|\n",
      "|  48302353|2013-09-01 06:05:40|          0.0|           1|      2|           0|       9|\n",
      "|  47502640|2013-09-01 05:57:40|          0.0|           1|      0|           0|       4|\n",
      "|  48999824|2013-09-01 06:12:40|          0.0|           1|      2|           0|       5|\n",
      "|  48267078|2013-09-01 06:05:20|       37.789|           1|      2|           0|       0|\n",
      "|  48601041|2013-09-01 06:08:40|          0.0|           1|      1|           0|       9|\n",
      "|  46072422|2013-09-01 05:43:20|          0.0|           1|      2|           0|       9|\n",
      "|  48368857|2013-09-01 06:06:20|        8.134|           1|      1|           0|       8|\n",
      "|  47270394|2013-09-01 05:55:20|          0.0|           1|      0|           0|       9|\n",
      "|  45971998|2013-09-01 05:42:20|          0.0|           1|      2|           0|       7|\n",
      "|  47469275|2013-09-01 05:57:20|          0.0|           1|      0|           0|       4|\n",
      "|  45938182|2013-09-01 05:42:00|          0.0|           1|      1|           0|       6|\n",
      "+----------+-------------------+-------------+------------+-------+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from readings\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res21: org.apache.spark.sql.streaming.StreamingQueryProgress =\n",
       "{\n",
       "  \"id\" : \"e37b32af-0c43-4e6e-9a3f-7794c6959505\",\n",
       "  \"runId\" : \"a93d4ae7-49d2-4c69-b225-8984a8317570\",\n",
       "  \"name\" : \"readings\",\n",
       "  \"timestamp\" : \"2020-03-04T09:20:14.163Z\",\n",
       "  \"batchId\" : 2,\n",
       "  \"numInputRows\" : 5249,\n",
       "  \"inputRowsPerSecond\" : 216.4536082474227,\n",
       "  \"processedRowsPerSecond\" : 77.15713655740115,\n",
       "  \"durationMs\" : {\n",
       "    \"addBatch\" : 67546,\n",
       "    \"getBatch\" : 0,\n",
       "    \"getEndOffset\" : 0,\n",
       "    \"queryPlanning\" : 348,\n",
       "    \"setOffsetRange\" : 0,\n",
       "    \"triggerExecution\" : 68030,\n",
       "    \"walCommit\" : 69\n",
       "  },\n",
       "  \"eventTime\" : {\n",
       "    \"avg\" : \"2013-09-01T06:48:14.932Z\",\n",
       "    \"max\" : \"2013-09-01T07:41:20.000Z\",\n",
       "    \"min\" : \"2013-09-01T05:38:20.000Z\",\n",
       "    \"watermark\" : \"2013-09-01T06:13:20.000Z\"\n",
       "  },\n",
       "  \"stateOperators\" : [ {\n",
       "    \"numRowsTotal\" ..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// readingsQuery.stop()\n",
    "readingsQuery.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res22: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Processing new data\",\n",
       "  \"isDataAvailable\" : true,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readingsQuery.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "statsQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@6bdb1f4a\n",
       "res4: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Getting offsets from KafkaV2[Subscribe[alert_2_stats]]\",\n",
       "  \"isDataAvailable\" : false,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val statsQuery = stats.writeStream.format(\"memory\").queryName(\"stats\").start()\n",
    "statsQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---------+---------+-------------------+\n",
      "|     day|hour|     mean|  std_dev|            last_ts|\n",
      "+--------+----+---------+---------+-------------------+\n",
      "|20130901|   5|1.8427234|4.0150304|2013-09-01 05:46:40|\n",
      "|20130901|   6| 9.980184| 40.45797|2013-09-01 06:22:00|\n",
      "|20130901|   9|29.620712| 86.62796|2013-09-01 09:00:20|\n",
      "|20130901|   7| 34.88543|  98.4204|2013-09-01 07:46:20|\n",
      "|20130901|   8|32.391823| 88.62441|2013-09-01 08:35:40|\n",
      "+--------+----+---------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Thread.sleep(10000)\n",
    "spark.sql(\"select * from stats\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: org.apache.spark.sql.streaming.StreamingQueryProgress =\n",
       "{\n",
       "  \"id\" : \"6e7bee32-b81b-4f7a-a9d8-e7b7fd1f6ff6\",\n",
       "  \"runId\" : \"22bb80e2-ea08-4f5f-93b7-eace29a7f6b1\",\n",
       "  \"name\" : \"stats\",\n",
       "  \"timestamp\" : \"2020-03-04T09:21:04.502Z\",\n",
       "  \"batchId\" : 1,\n",
       "  \"numInputRows\" : 1,\n",
       "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
       "  \"processedRowsPerSecond\" : 0.053969453289438175,\n",
       "  \"durationMs\" : {\n",
       "    \"addBatch\" : 17907,\n",
       "    \"getBatch\" : 1,\n",
       "    \"getEndOffset\" : 0,\n",
       "    \"queryPlanning\" : 121,\n",
       "    \"setOffsetRange\" : 1,\n",
       "    \"triggerExecution\" : 18529,\n",
       "    \"walCommit\" : 36\n",
       "  },\n",
       "  \"stateOperators\" : [ ],\n",
       "  \"sources\" : [ {\n",
       "    \"description\" : \"KafkaV2[Subscribe[alert_2_stats]]\",\n",
       "    \"startOffset\" : {\n",
       "      \"alert_2_stats\" : {\n",
       "        \"0\" : 0\n",
       "      }\n",
       "    },\n",
       "    \"endOffset\" : {\n",
       "      \"alert_2_stats\" : {\n",
       "        \"..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// statsQuery.stop()\n",
    "statsQuery.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res20: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Processing new data\",\n",
       "  \"isDataAvailable\" : true,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statsQuery.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Both Data Streams\n",
    "\n",
    "\n",
    "We can only compare the readings with the past statistics. The `reading_ts` value of `readings_prepared` stream needs to be bigger than the `last_ts` value of the `alert_2_stats` stream. We need to ensure we are comparing the stats from the right `day` and `hour`.\n",
    "\n",
    "After joining, we need to ensure that we only compare the readings with their latest past statistics.\n",
    "\n",
    "The `alert_2_stats` stream is assumed to be slower to arrive compared to the readings_prepared stream, since it needs to wait at least 2 records to start calculating, and the default write trigger is every 1 minute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "anomaly: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [message_id: string, reading_ts: timestamp ... 10 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Anomaly detection is done by getting the latest std_dev and mean value\n",
    "// Then act on it by comparing to the \n",
    "val anomaly = readings\n",
    "    .withWatermark(\"reading_ts\", joinWatermarkTime)\n",
    "    .join(\n",
    "        stats.withWatermark(\"last_ts\", joinWatermarkTime),\n",
    "        // Join conditions\n",
    "        $\"reading_ts\" > $\"last_ts\" &&\n",
    "        hour($\"reading_ts\") === $\"hour\" &&\n",
    "        date_format($\"reading_ts\", \"yyyyMMdd\") === $\"day\",\n",
    "        // Join type\n",
    "        \"inner\"\n",
    "    )\n",
    "    .filter($\"reading_value\" > $\"mean\" + $\"std_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peekQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@48038837\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val peekQuery = anomaly\n",
    "    .writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"anomaly\")\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------+------------+-------+------------+--------+--------+----+---------+--------+-------------------+\n",
      "|message_id|         reading_ts|reading_value|reading_type|plug_id|household_id|house_id|     day|hour|     mean| std_dev|            last_ts|\n",
      "+----------+-------------------+-------------+------------+-------+------------+--------+--------+----+---------+--------+-------------------+\n",
      "|  69012174|2013-09-01 09:33:20|      141.553|           1|      1|           0|       1|20130901|   9|29.620712|86.62796|2013-09-01 09:00:20|\n",
      "|  69247340|2013-09-01 09:35:40|      396.696|           1|      1|           0|       8|20130901|   9|29.620712|86.62796|2013-09-01 09:00:20|\n",
      "|  66916542|2013-09-01 09:12:20|      405.686|           1|      1|           0|       8|20130901|   9|29.620712|86.62796|2013-09-01 09:00:20|\n",
      "|  70376478|2013-09-01 09:47:00|      411.481|           1|      1|           0|       8|20130901|   9|29.620712|86.62796|2013-09-01 09:00:20|\n",
      "|  67882142|2013-09-01 09:22:00|      413.912|           1|      1|           0|       8|20130901|   9|29.620712|86.62796|2013-09-01 09:00:20|\n",
      "|  71066196|2013-09-01 09:54:00|      402.398|           1|      1|           0|       8|20130901|   9|29.620712|86.62796|2013-09-01 09:00:20|\n",
      "|  66216158|2013-09-01 09:05:20|      142.332|           1|      1|           0|       1|20130901|   9|29.620712|86.62796|2013-09-01 09:00:20|\n",
      "|  68446113|2013-09-01 09:27:40|      144.583|           1|      1|           0|       1|20130901|   9|29.620712|86.62796|2013-09-01 09:00:20|\n",
      "|  70628222|2013-09-01 09:49:40|       430.99|           1|      1|           0|       8|20130901|   9|29.620712|86.62796|2013-09-01 09:00:20|\n",
      "|  69411510|2013-09-01 09:37:20|      147.117|           1|      1|           0|       1|20130901|   9|29.620712|86.62796|2013-09-01 09:00:20|\n",
      "|  69644910|2013-09-01 09:39:40|      140.018|           1|      1|           0|       1|20130901|   9|29.620712|86.62796|2013-09-01 09:00:20|\n",
      "|  66848675|2013-09-01 09:11:40|      133.908|           1|      1|           0|       1|20130901|   9|29.620712|86.62796|2013-09-01 09:00:20|\n",
      "|  67980263|2013-09-01 09:23:00|      155.181|           1|      1|           0|       1|20130901|   9|29.620712|86.62796|2013-09-01 09:00:20|\n",
      "|  67513908|2013-09-01 09:18:20|      127.047|           1|      1|           0|       1|20130901|   9|29.620712|86.62796|2013-09-01 09:00:20|\n",
      "|  69946654|2013-09-01 09:42:40|      421.942|           1|      1|           0|       8|20130901|   9|29.620712|86.62796|2013-09-01 09:00:20|\n",
      "|  66149956|2013-09-01 09:04:40|      142.714|           1|      1|           0|       1|20130901|   9|29.620712|86.62796|2013-09-01 09:00:20|\n",
      "|  66383306|2013-09-01 09:07:00|      155.998|           1|      1|           0|       1|20130901|   9|29.620712|86.62796|2013-09-01 09:00:20|\n",
      "|  71130488|2013-09-01 09:54:40|      174.261|           1|      1|           0|       1|20130901|   9|29.620712|86.62796|2013-09-01 09:00:20|\n",
      "|  67681018|2013-09-01 09:20:00|      164.513|           1|      1|           0|       1|20130901|   9|29.620712|86.62796|2013-09-01 09:00:20|\n",
      "|  71301964|2013-09-01 09:56:20|      380.004|           1|      1|           0|       8|20130901|   9|29.620712|86.62796|2013-09-01 09:00:20|\n",
      "+----------+-------------------+-------------+------------+-------+------------+--------+--------+----+---------+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from anomaly\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: org.apache.spark.sql.streaming.StreamingQueryProgress =\n",
       "{\n",
       "  \"id\" : \"0e3c0ee6-6d96-43d4-8963-c147ef0bcedb\",\n",
       "  \"runId\" : \"dece0a41-b90b-46cf-8fb2-aa94949c6b59\",\n",
       "  \"name\" : \"anomaly\",\n",
       "  \"timestamp\" : \"2020-03-04T09:19:59.052Z\",\n",
       "  \"batchId\" : 0,\n",
       "  \"numInputRows\" : 49,\n",
       "  \"processedRowsPerSecond\" : 0.6581333190066216,\n",
       "  \"durationMs\" : {\n",
       "    \"addBatch\" : 72221,\n",
       "    \"getBatch\" : 1,\n",
       "    \"getEndOffset\" : 0,\n",
       "    \"queryPlanning\" : 1565,\n",
       "    \"setOffsetRange\" : 495,\n",
       "    \"triggerExecution\" : 74453,\n",
       "    \"walCommit\" : 82\n",
       "  },\n",
       "  \"eventTime\" : {\n",
       "    \"avg\" : \"2013-09-01T06:47:10.000Z\",\n",
       "    \"max\" : \"2013-09-01T07:05:00.000Z\",\n",
       "    \"min\" : \"2013-09-01T06:22:00.000Z\",\n",
       "    \"watermark\" : \"1970-01-01T00:00:00.000Z\"\n",
       "  },\n",
       "  \"stateOperators\" : [ {\n",
       "    \"numRowsTotal\" : 22,\n",
       "    \"numRowsUpdated\" : 22,\n",
       "    \"memor..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peekQuery.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Processing new data\",\n",
       "  \"isDataAvailable\" : true,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peekQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "// peekQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to BigQuery\n",
    "\n",
    "Write to BigQuery once a minute. BigQuery inserts works better with batched data and this ensures that we give enough time for `alert_2_stats` to accumulate. The [BigQuery connector](https://github.com/GoogleCloudDataproc/spark-bigquery-connector) is installed during cluster setup and loaded automatically when spark-shell session is initiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ingestQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@5716fb32\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ingestQuery = anomaly\n",
    "    .writeStream\n",
    "    .trigger(Trigger.ProcessingTime(outputTriggerTime))\n",
    "    .foreachBatch{ \n",
    "        (batchDF: DataFrame, batchId: Long) =>\n",
    "            batchDF.write.format(\"bigquery\")\n",
    "                .option(\"table\", bigQueryTargetTable)\n",
    "                .option(\"temporaryGcsBucket\", bigQueryTempBucket)\n",
    "                .mode(SaveMode.Append)\n",
    "                .save()\n",
    "    }.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res26: org.apache.spark.sql.streaming.StreamingQueryProgress =\n",
       "{\n",
       "  \"id\" : \"5e4d4f11-e20b-4cf5-93d1-e40e6b62a316\",\n",
       "  \"runId\" : \"f0e6aa39-58f5-4d09-8db1-22725b4f5db3\",\n",
       "  \"name\" : null,\n",
       "  \"timestamp\" : \"2020-03-04T09:24:34.974Z\",\n",
       "  \"batchId\" : 2,\n",
       "  \"numInputRows\" : 23333,\n",
       "  \"inputRowsPerSecond\" : 199.5057886006464,\n",
       "  \"processedRowsPerSecond\" : 229.6148320179496,\n",
       "  \"durationMs\" : {\n",
       "    \"addBatch\" : 101243,\n",
       "    \"getBatch\" : 1,\n",
       "    \"getEndOffset\" : 0,\n",
       "    \"queryPlanning\" : 311,\n",
       "    \"setOffsetRange\" : 5,\n",
       "    \"triggerExecution\" : 101618,\n",
       "    \"walCommit\" : 26\n",
       "  },\n",
       "  \"eventTime\" : {\n",
       "    \"avg\" : \"2013-09-01T19:40:46.472Z\",\n",
       "    \"max\" : \"2013-09-01T23:06:00.000Z\",\n",
       "    \"min\" : \"2013-09-01T15:59:00.000Z\",\n",
       "    \"watermark\" : \"2013-09-01T14:00:40.000Z\"\n",
       "  },\n",
       "  \"stateOperators\" : [ {\n",
       "    \"numRowsTotal\" : 1..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingestQuery.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res27: org.apache.spark.sql.streaming.StreamingQueryStatus =\n",
       "{\n",
       "  \"message\" : \"Processing new data\",\n",
       "  \"isDataAvailable\" : true,\n",
       "  \"isTriggerActive\" : true\n",
       "}\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingestQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "// ingestQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}